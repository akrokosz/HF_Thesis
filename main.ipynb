{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import datetime\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "import scipy.special\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import stock_indicators as sa\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import pandas as pd\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "import yfinance as yf\n",
    "from stock_indicators import Quote\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from stock_indicators.indicators.common.enums import PeriodSize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)"
   ],
   "id": "a391dd87a6032889"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(0)"
   ],
   "id": "59a8d087894a72b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "funds_w_names = defaultdict(pd.DataFrame)",
   "id": "69e44f88899a41ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "etf_names = ['IE0005042456', 'EDMU.SW', 'EDG2.L', '36BA.DE', 'CBUS.DE', 'EWSA.AS','IE00B0M62X26','IE00B14X4Q57','IE00B1XNHC34','IE00B3FH7618','IE00B3ZW0K18','IE00B52MJY50','IE00B5M4WH52','IE00B66F4759','EMBE.L', 'DTLE.L','IE00BDFK1573','IBC5.DE','IE00BFNM3G45','IE00BHZPJ015','IE00BHZPJ452','IE00BHZPJ783',\n",
    "'IE00BLDGH553','IE00BMG6Z448','IE00BYYHSM20','IE00BYZTVT56','IE00BZ173V67','IE00BZ1NCS44','ISVIF', \"IESE.AS\", \"UEEF.DE\"]"
   ],
   "id": "f7ce3e561814d8d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(etf_names)",
   "id": "86c486b5a4108674"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for fund in etf_names:\n",
    "    print(fund)\n",
    "    a = (pd.DataFrame(yf.Ticker(fund).history(start=\"2023-01-03\", end=\"2024-01-01\")))\n",
    "    a.index = pd.to_datetime(a.index)\n",
    "    a.index = a.index.normalize()\n",
    "    full_date_range = pd.date_range(start=a.index.min(), end=a.index.max(), freq=\"D\")\n",
    "    a = a.reindex(full_date_range)\n",
    "\n",
    "    a[\"Return\"] = a[\"Close\"].pct_change()\n",
    "    a[\"Rolling Volatility\"] = a[\"Return\"].rolling(7).std()\n",
    "\n",
    "    a.fillna(method=\"bfill\", inplace=True)\n",
    "    if fund != 'ISVIF':\n",
    "        a.drop([\"Dividends\", \"Stock Splits\", \"Capital Gains\"], inplace=True, axis=1)\n",
    "    else:\n",
    "        a.drop([\"Dividends\", \"Stock Splits\"], inplace=True, axis=1)\n",
    "    funds_w_names[fund] = a"
   ],
   "id": "5bb5e4f57a144f09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import pickle\n",
    "# pickle.dump(funds_w_names, open('funds_w_names.pkl', 'wb'))"
   ],
   "id": "bec790365c3d5b59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import pickle\n",
    "# pickle.dump(funds_w_names, open('funds_w_names_2.pkl', 'wb'))"
   ],
   "id": "9dbd39403048646e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pickle\n",
    "funds_w_names = pd.read_pickle('funds_w_names_2.pkl')"
   ],
   "id": "50be1a21d8b28f74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def portfolio_factory(etfs: list):\n",
    "    giga_fund = pd.DataFrame()\n",
    "    for c, ff in enumerate(etfs):\n",
    "        f = ff.copy()\n",
    "        # Ensure index is datetime\n",
    "        f.index = pd.to_datetime(f.index, utc=True)\n",
    "        if f.index.hour[0]==0:\n",
    "            f.index = f.index+datetime.timedelta(hours=-1)\n",
    "        f.index = f.index.normalize()\n",
    "\n",
    "        # Sort by datetime index (important for time series models)\n",
    "        f = f.sort_index()\n",
    "\n",
    "        # Add time_idx relative to the start of the individual ETF\n",
    "        f[\"time_idx\"] = (f.index - f.index.min()).days\n",
    "\n",
    "        # Add unique group_id\n",
    "        f[\"group_id\"] = c\n",
    "\n",
    "        # Reset index to avoid issues with overlapping indices\n",
    "        f = f.reset_index()\n",
    "\n",
    "        # Concatenate into the master DataFrame\n",
    "        giga_fund = pd.concat([giga_fund, f], ignore_index=True)\n",
    "    return giga_fund"
   ],
   "id": "b069dd37abd77bc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "funds_w_names[\"EWSA.AS\"]",
   "id": "b854d39bbd597bcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pd.options.display.date_yearfirst = True  # Ensures year-first format\n",
    "pd.options.display.float_format = '{:.2f}'.format  # Example for float formatting"
   ],
   "id": "668a48ed77200b88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def technical_indicators_factory(etfs: list, names: list):\n",
    "    indicators_per_fund = defaultdict(pd.DataFrame)\n",
    "    figs = []\n",
    "\n",
    "    for id, f in enumerate(etfs):\n",
    "        quotes_for_f = [\n",
    "            Quote(\n",
    "                date=row.Index,\n",
    "                open=row.Open,\n",
    "                high=row.High,\n",
    "                low=row.Low,\n",
    "                close=row.Close,\n",
    "                volume=row.Volume\n",
    "            )\n",
    "            for row in f.itertuples()]\n",
    "\n",
    "\n",
    "        macd_for_f = sa.indicators.get_macd(quotes=quotes_for_f)\n",
    "        valid_macd_for_f = [\n",
    "            (result.date, result.macd, result.signal, result.histogram)\n",
    "            for result in macd_for_f\n",
    "            if result.macd is not None and result.signal is not None and result.histogram is not None\n",
    "        ]\n",
    "        macd_dates, macd_values, macd_signal, macd_histogram = zip(*valid_macd_for_f)\n",
    "\n",
    "\n",
    "        rsi_for_f = sa.indicators.get_rsi(quotes=quotes_for_f)\n",
    "        valid_rsi_for_f = [\n",
    "            (result.date, result.rsi)\n",
    "            for result in rsi_for_f\n",
    "            if result.date is not None and result.rsi is not None\n",
    "        ]\n",
    "        rsi_dates, rsi_values = zip(*valid_rsi_for_f)\n",
    "\n",
    "\n",
    "        bb_for_f = sa.indicators.get_bollinger_bands(quotes=quotes_for_f)\n",
    "        valid_bb_for_f = [\n",
    "            (result.date, result.lower_band, result.upper_band)\n",
    "            for result in bb_for_f\n",
    "            if result.date is not None and result.lower_band is not None\n",
    "        ]\n",
    "        bb_dates, bb_lower, bb_upper = zip(*valid_bb_for_f)\n",
    "\n",
    "\n",
    "        vwap_for_f = sa.indicators.get_vwap(quotes=quotes_for_f)\n",
    "        valid_vwap_for_f = [\n",
    "            (result.date, result.vwap)\n",
    "            for result in vwap_for_f\n",
    "            if result.date is not None and result.vwap is not None\n",
    "        ]\n",
    "        vwap_dates, vwap_values = zip(*valid_vwap_for_f)\n",
    "\n",
    "\n",
    "\n",
    "        pp_for_f = sa.indicators.get_pivot_points(quotes=quotes_for_f, window_size=PeriodSize.DAY)\n",
    "        valid_pp_for_f = [\n",
    "            (result.date, result.pp, result.r1, result.r2, result.r3, result.r4, result.s1, result.s2, result.s3, result.s4)\n",
    "            for result in pp_for_f\n",
    "            if result.date is not None and result.pp is not None\n",
    "        ]\n",
    "        pp_dates, pp, r1, r2, r3, r4, s1, s2, s3, s4 = zip(*valid_pp_for_f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        fig, axes = plt.subplots(5, 1, figsize=(12,6))\n",
    "\n",
    "\n",
    "        axes[0].plot(macd_dates[-20:], macd_values[-20:], label='MACD_values', linewidth=2)\n",
    "        axes[0].plot(macd_dates[-20:], macd_signal[-20:], label='MACD_signal', linewidth=2)\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        axes[1].plot(rsi_dates[-20:], rsi_values[-20:], label='RSI', linewidth=2)\n",
    "        axes[1].legend(loc='best')\n",
    "\n",
    "        axes[2].plot(bb_dates[-20:], bb_lower[-20:], label='BB_lower', linewidth=2)\n",
    "        axes[2].plot(bb_dates[-20:], bb_upper[-20:], label='BB_upper', linewidth=2)\n",
    "        axes[2].legend(loc='best')\n",
    "\n",
    "        axes[3].plot(vwap_dates[-20:], vwap_values[-20:], label='vwap', linewidth=2)\n",
    "        axes[3].legend(loc='best')\n",
    "\n",
    "        axes[4].plot(pp_dates[-20:], pp[-20:], label='pivot points', linewidth=2)\n",
    "        axes[4].plot(pp_dates[-20:], r1[-20:], label='r1', linewidth=2)\n",
    "        axes[4].plot(pp_dates[-20:], r4[-20:], label='r4', linewidth=2)\n",
    "        axes[4].plot(pp_dates[-20:], s1[-20:], label='s1', linewidth=2)\n",
    "        axes[4].plot(pp_dates[-20:], s4[-20:], label='s4', linewidth=2)\n",
    "        axes[4].legend(loc='best')\n",
    "\n",
    "\n",
    "        figs.append(fig)\n",
    "\n",
    "        f['Bullish'] = [1 if x.histogram and x.histogram > 0 else 0 for x in macd_for_f]\n",
    "        f['Bearish'] = [1 if x.histogram and x.histogram < 0 else 0 for x in macd_for_f]\n",
    "        f['isOverbought'] = [1 if x.rsi and x.rsi > 70 else 0 for x in rsi_for_f ]\n",
    "        f['isOversold'] = [1 if x.rsi and x.rsi < 30 else 0 for x in rsi_for_f ]\n",
    "\n",
    "\n",
    "        f.fillna(method=\"ffill\", inplace=True)\n",
    "        indicators_per_fund[names[id]]= f\n",
    "    return indicators_per_fund, figs"
   ],
   "id": "13b64b34be0122f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def fit_predict(pred_len, epochs: int, batch_size: int, lr: float, dropout: float, df: pd.DataFrame, independent_variables: list, target=\"Close\", training_cutoff_idx: pd.Timestamp=333):\n",
    "    training = TimeSeriesDataSet(\n",
    "        df[lambda x: x.time_idx <= training_cutoff_idx],  # Use the determined cutoff index\n",
    "        time_idx=\"time_idx\",                              # Sequential time index\n",
    "        target=target,                                    # Target variable\n",
    "        group_ids=[\"group_id\"],                           # Group identifier\n",
    "        min_encoder_length=22,                             # Minimum input sequence length\n",
    "        max_encoder_length=365,                            # Maximum input sequence length\n",
    "        min_prediction_length=1,                         # Minimum forecast length\n",
    "        max_prediction_length=pred_len,                   # Maximum forecast length (31 for December)\n",
    "        static_reals=[],                                  # No static real variables\n",
    "        time_varying_known_reals= independent_variables,  # Known inputs\n",
    "        time_varying_unknown_reals=[target],              # Target variable\n",
    "        target_normalizer=GroupNormalizer(groups=[\"group_id\"], transformation=\"softplus\"),\n",
    "        add_relative_time_idx=True,                       # Add relative time index\n",
    "        add_target_scales=True,                           # Scale the target variable\n",
    "        add_encoder_length=True,                          # Include encoder length feature\n",
    "        allow_missing_timesteps=False,                    # Ensure no missing steps\n",
    "    )\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(\n",
    "        training,\n",
    "        df,\n",
    "        predict=True,\n",
    "        stop_randomization=True                             # Ensures no randomization in validation dataset\n",
    "    )\n",
    "\n",
    "    batch_size = batch_size\n",
    "\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=lr,              # Learning rate\n",
    "        hidden_size=16,                  # Model hidden size\n",
    "        attention_head_size=4,           # Number of attention heads\n",
    "        dropout=dropout,                     # Dropout rate\n",
    "        hidden_continuous_size=8,        # Hidden size for continuous variables\n",
    "        output_size=7,                   # Output quantiles (e.g., 10th to 90th percentile)\n",
    "        loss=QuantileLoss(),             # Loss function\n",
    "        log_interval=10,                 # Log every 10 batches\n",
    "        reduce_on_plateau_patience=4,    # Reduce learning rate on plateau\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints/\",\n",
    "        filename=\"tft-model-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\",  #\n",
    "        mode=\"min\",  # minimize validation loss\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=8, verbose=False, mode=\"min\")\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        # logger=logger,\n",
    "        max_epochs=epochs,\n",
    "        accelerator='cuda',\n",
    "        devices=\"auto\",\n",
    "        gradient_clip_val=0.1,\n",
    "        callbacks=[early_stop_callback, lr_logger],#, checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    trainer.fit(tft, train_dataloader, val_dataloader)\n",
    "    raw_predictions = tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    predicted_median_np = raw_predictions.output.prediction[0, :, 1].detach().cpu().numpy()\n",
    "    return predicted_median_np"
   ],
   "id": "a73726601f07c1fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def huber_loss(actual_val, pred_val, delta=5.0):\n",
    "    err = np.abs(actual_val - pred_val)\n",
    "    return np.mean(np.where( err <= delta, 0.5 * err**2, delta * (err - 0.5 * delta)))"
   ],
   "id": "accbdd6298d350bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scipy.special import huber\n",
    "def metrics_and_plt(df: pd.DataFrame, preds:list, date_range:pd.date_range, target=\"Close\", date = \"2023-12-01\"):\n",
    "    actual_values = df[target][date:]\n",
    "    mse = mean_squared_error(actual_values, preds)\n",
    "    mape = mean_absolute_percentage_error(actual_values, preds)\n",
    "    huber = huber_loss(actual_values, preds)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(date_range, actual_values, marker=\"x\", label=\"Actual Close\", color=\"orange\")\n",
    "    plt.plot(date_range, preds, marker=\"o\", label=\"Predicted Median\")\n",
    "    plt.title(\"Predicted Values for December 01 to December 29, 2023\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Predicted Value\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return mse, plt, mape, huber"
   ],
   "id": "2294e200c1133d6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BASELINE MODEL",
   "id": "f4d1b0a76305dd10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import GARCH"
   ],
   "id": "77822c910f26ea5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "f1 = funds_w_names['EWSA.AS'][\"Close\"][:-30]",
   "id": "198acc50e20ad0e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = ARIMA(f1, order=(1, 1, 1))\n",
    "model_fit = model.fit()\n",
    "arima_forecast = model_fit.forecast(steps=30)\n",
    "print(\"ARIMA Forecast for the next 30 periods:\", forecast)"
   ],
   "id": "860c1ffc3da4878"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "arima_mse = mean_squared_error(funds_w_names[\"EWSA.AS\"][\"Close\"][-30:], arima_forecast)",
   "id": "389075535c15f45a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sf = StatsForecast(models=[GARCH()], freq='D')\n",
    "f1=pd.DataFrame(f1)\n",
    "f1['ds'] = pd.to_datetime(f1.index)\n",
    "f1.rename(columns={'Close': 'y'}, inplace=True)\n",
    "f1['unique_id']=1"
   ],
   "id": "e1637a688b10409b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "garch_forecast = sf.forecast(h=30, df=f1)\n",
    "\n",
    "print(garch_forecast)"
   ],
   "id": "a0e568249dc3d3e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "garch_mse = mean_squared_error(funds_w_names[\"EWSA.AS\"][\"Rolling Volatility\"][-30:], garch_forecast[\"GARCH(1,1)\"])",
   "id": "3afabbf8a7e869d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "NO INDICATORS, ONE ETF",
   "id": "25ce8bb1bd85414b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#IE00B52MJY50\n",
    "IE = portfolio_factory(etfs=[funds_w_names['EWSA.AS']])"
   ],
   "id": "1319cb3e768626fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE",
   "id": "3699d369acfa639"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_preds = fit_predict(pred_len=29, epochs=100, batch_size=128, lr=1e-3, dropout=0.1, df=IE, independent_variables=[\"Open\", \"High\", \"Low\", \"Volume\"])",
   "id": "f22b7a552d6230ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_mse, IE_plt = metrics_and_plt(IE, IE_preds, pd.date_range(start=\"2023-12-01\", end=\"2023-12-29\"))",
   "id": "bc9e95660f8fe9e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_mse",
   "id": "f53f6ddc0a0a9004"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ADDING INDICATORS",
   "id": "356b90b6eec4c9b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_indicators, IE_indicators_plt1 = technical_indicators_factory([funds_w_names['EWSA.AS']], ['EWSA.AS'])",
   "id": "5a5b2b62075d6861"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "analiza = IE_indicators[\"EWSA.AS\"][[\"isOversold\", \"isOverbought\", \"Bullish\", \"Bearish\", \"Return\"]]",
   "id": "93dcdbe2bc3dcc2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "corr1 = analiza.corr()\n",
    "sns.heatmap(corr1, annot=True, cmap=\"coolwarm\", fmt=\".2f\")"
   ],
   "id": "57d8d9dd9c2f4157"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "correlations = IE_indicators[\"EWSA.AS\"].corr()",
   "id": "2cc98a5dee133b36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(correlations, annot=True, cmap=\"coolwarm\", fmt=\".2f\")"
   ],
   "id": "47e92a7e48398ed3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_indicators_preds = fit_predict(pred_len=29, epochs=220, batch_size=128, lr=1e-3, dropout=0.1, df=IE_indicators[\"EWSA.AS\"], independent_variables=[\"Open\", \"High\", \"Low\", \"isOverbought\", \"Bullish\"])",
   "id": "247a5ef83374325e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_indicators_mse, IE_indicators_plt2= metrics_and_plt(IE, IE_indicators_preds, pd.date_range(start=\"2023-12-01\", end=\"2023-12-29\"))",
   "id": "d691ce130f83a55a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_indicators_mse",
   "id": "63c52fe93eb2bea7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ENSAMBLE MODEL 2: REMAINDERS",
   "id": "322931826d25c384"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "resztki = abs(IE_indicators_preds  - IE_indicators[\"EWSA.AS\"][\"Close\"][\"2023-12-01\":])",
   "id": "e45377b5c0f9e7fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "resztki",
   "id": "c66859dbefe7d562"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_scary = IE_indicators[\"EWSA.AS\"][333:].copy()\n",
    "df_scary[\"Remainders\"] = resztki"
   ],
   "id": "94891e9d356d7493"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_scary",
   "id": "e1f611f07f9b313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_scary.index = pd.to_datetime(df_scary.index)  # ensure index is datetime if not already\n",
    "df_scary[\"time_idx\"] = (df_scary.index.date - df_scary.index.date[0]).astype(\"timedelta64[D]\").astype(int)\n",
    "df_scary['group_id'] = 0"
   ],
   "id": "40335cfaf637bdb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_model2 = fit_predict(pred_len=7,epochs=220, batch_size=128, lr=1e-3, dropout=0.1, df=df_scary, independent_variables=[\"Open\", \"High\", \"Low\", \"isOverbought\", \"Bullish\"], target=\"Remainders\")",
   "id": "cf83af02435da74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_model2_mse, IE_model2_plt= metrics_and_plt(df_scary, IE_model2, pd.date_range(start=\"2023-12-23\", end=\"2023-12-29\"), target=\"Remainders\", date=\"2023-12-23\")",
   "id": "7a1eaaff061d8325"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_preds = IE_indicators_preds.copy()[-7:] + IE_model2",
   "id": "d884f5372acfb494"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_mse = mean_squared_error(IE_indicators[\"EWSA.AS\"][\"Close\"][-7:], final_preds)",
   "id": "6ade75cf498620a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_mse",
   "id": "79483ffbafa9578e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "FULL HEDGE PORTFOLIO : EKSPERYMRENT",
   "id": "9e238bdd4b15a16c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "funds_for_hedge = [\"ISVIF\", \"IE00BHZPJ783\", \"36BA.DE\", \"IE00B3ZW0K18\", \"IE00BMG6Z448\"]",
   "id": "b7d498ab9fa0ff4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "funds_with_indics = []\n",
    "for fund in funds_for_hedge:\n",
    "    funds_with_indics.append(funds_w_names[fund].copy())"
   ],
   "id": "375e4ea7a1dbcb32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "fund_name_fund_df, sum_plt = technical_indicators_factory(funds_with_indics, funds_for_hedge)",
   "id": "25f54171d06670cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "GIGAFUND = portfolio_factory(etfs=[fund_name_fund_df[k] for k in (funds_for_hedge)])",
   "id": "b7988cd80fac6dc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "GIGAFUND_NORM = GIGAFUND.copy()\n",
    "\n",
    "columns_to_normalize = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Return\", \"Rolling Volatility\"]\n",
    "\n",
    "GIGAFUND_NORM[columns_to_normalize] = (\n",
    "    GIGAFUND_NORM[columns_to_normalize]\n",
    "    .apply(lambda col: (col - col.min()) / (col.max() - col.min()), axis=0)\n",
    ")"
   ],
   "id": "5f26c7e2c6ea7acc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "GIGAFUND_NORM",
   "id": "3e0fd73db4480f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "duplicate_rows = GIGAFUND_NORM[GIGAFUND_NORM.duplicated(subset=[\"time_idx\", \"group_id\"], keep=False)]\n",
    "print(duplicate_rows)"
   ],
   "id": "19885e856659059"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_indicators_preds = fit_predict(pred_len=29, epochs=220, batch_size=128, lr=1e-3, dropout=0.1, df=GIGAFUND_NORM, independent_variables=[\"Open\", \"High\", \"Low\", \"isOverbought\", \"Bullish\"])",
   "id": "6c323e0716afd92c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MONTER CARLOS PREDICTING RISK",
   "id": "b9020b03c74c0090"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calc_drift(fund):\n",
    "    log_returns = np.log(1+fund[\"Close\"].pct_change())\n",
    "    log_returns.fillna(value=0, inplace=True)\n",
    "    avg_pdr = log_returns.mean()\n",
    "    var = log_returns.var()\n",
    "    drift = avg_pdr-(.5*var)\n",
    "\n",
    "    return drift"
   ],
   "id": "179dec9b3f1cbcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def monte_carlo_sim(fund):\n",
    "    drift = calc_drift(fund)\n",
    "    log_returns = np.log(1+fund[\"Close\"].pct_change())\n",
    "    log_returns.fillna(value=0, inplace=True)\n",
    "    stdev=log_returns.std()\n",
    "    days=362\n",
    "    trials=100\n",
    "    Z = norm.ppf(np.random.rand(days,trials))\n",
    "    daily_returns=np.exp(np.array(drift) + np.array(stdev) * Z)\n",
    "    price_paths = np.zeros_like(daily_returns)\n",
    "    price_paths[0] = fund[\"Close\"].iloc[-1]\n",
    "    for i in range(1,days):\n",
    "        price_paths[i] = price_paths[i-1] * daily_returns[i]\n",
    "    return price_paths"
   ],
   "id": "40c69d9d11cfb4a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "drifts=calc_drift(funds_w_names[\"EWSA.AS\"])\n",
    "pp=monte_carlo_sim(funds_w_names[\"EWSA.AS\"])"
   ],
   "id": "b795df9f1dd24c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def volatility_mc(price_paths: pd.DataFrame):\n",
    "    pp_df = pd.DataFrame(price_paths)\n",
    "    pp_df= pp_df.pct_change()\n",
    "    pp_df.fillna(value=0, inplace=True)\n",
    "    pp_df=pp_df.rolling(7).std()\n",
    "    pp_df.fillna(method=\"bfill\", inplace=True)\n",
    "    return np.mean(pp_df)"
   ],
   "id": "461baf78140662c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "volatility_mc(pp)",
   "id": "ed91a8e5b979392e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(pp)"
   ],
   "id": "17e722c1327143d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TEZ DO KOSZA",
   "id": "fd5d8ebe88635182"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "list_of_5_df_funds = []",
   "id": "1a8d5e37d8de8c5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def nearest_positive_definite(matrix):\n",
    "    P = matrix.copy()\n",
    "    eigvals, eigvecs = np.linalg.eigh(P)\n",
    "    eigvals[eigvals < 0] = 1e-10\n",
    "    return eigvecs @ np.diag(eigvals) @ eigvecs.T"
   ],
   "id": "ef02ec21ac663474"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "variances = {}\n",
    "covs_means = []\n",
    "for k,v in funds_w_names.items():\n",
    "    variances[k] = v[\"Rolling Volatility\"]\n",
    "variances = pd.DataFrame(variances)\n",
    "r = variances.pct_change()\n",
    "covariance_matrix = r.cov()\n",
    "mean_returns = r.mean()"
   ],
   "id": "65eec6eee477a92c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not np.all(np.linalg.eigvals(covariance_matrix) > 0):\n",
    "    print(\"Covariance matrix not positive definite. Adding regularization.\")\n",
    "    covariance_matrix = nearest_positive_definite(covariance_matrix)"
   ],
   "id": "9ed3d7fdc069a671"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#to be filled with real data\n",
    "portfolio_weights = np.random.random(len(mean_returns)) # distribution of etfs in portfolio\n",
    "portfolio_weights /= np.sum(portfolio_weights)"
   ],
   "id": "4c52ddafd3fa7c6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "initial_portfoloio = 0\n",
    "for i, v in enumerate(funds_w_names.values()):\n",
    "    initial_portfoloio += portfolio_weights[i] * v[\"Close\"][i]"
   ],
   "id": "5ffd0105a667ab42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "days = 60\n",
    "simulations = 100\n",
    "portfolio_sims = np.full(shape=(days, simulations), fill_value=0.0) # default\n",
    "mean_matrix = np.full(shape=(days, len(portfolio_weights)), fill_value=mean_returns).T # days x all etfs"
   ],
   "id": "f0c4659908ecd478"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "initial_portfoloio",
   "id": "80f925d4c11237f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for s in range(0, simulations):\n",
    "    Z = np.random.normal(size=(days, len(portfolio_weights)))\n",
    "    L = np.linalg.cholesky(covariance_matrix)\n",
    "    daily_returns = mean_matrix + np.inner(L, Z)\n",
    "    portfolio_sims[:, s] = np.cumprod(np.inner(portfolio_weights, daily_returns.T)+1)*initial_portfoloio\n",
    "\n",
    "plt.plot(portfolio_sims)\n",
    "plt.ylabel(\"Portfolio Simulations\")"
   ],
   "id": "93cbcada5b4054ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "expected volatility",
   "id": "45378aa24a58a013"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "portfolio_returns = portfolio_sims[-1] / portfolio_sims[0] - 1  # final value over initial value gives portfolio returns\n",
    "portfolio_volatility = np.std(portfolio_returns)\n",
    "print(f\"Portfolio Volatility: {portfolio_volatility:.4f}\")"
   ],
   "id": "146581d6c964adac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(portfolio_sims)\n",
    "plt.title(\"Monte Carlo Simulations of Portfolio Value\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Portfolio Value\")\n",
    "plt.show()"
   ],
   "id": "6d62fd1a6ac48f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "expected return",
   "id": "30e48f45015ad86a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "discounted_prices = [sim[-1]*0.96 for sim in portfolio_sims]",
   "id": "d6b7b25ab67585e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "avg = np.average(discounted_prices)",
   "id": "ce88f7402221e552"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MONTE CARLO EVALUATION FUNCITON",
   "id": "90fb4a112323f0ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test_robustness(paths, num_trials, tol=0.01):\n",
    "    means = []\n",
    "    for i in range(num_trials):\n",
    "        np.random.seed(i)\n",
    "        new_paths = np.random.choice(paths, size=len(paths), replace=True)\n",
    "        means.append(np.mean(new_paths))\n",
    "    glob_mean = np.mean(means)\n",
    "    devs = np.abs(np.array(means) - glob_mean)\n",
    "    return max(0, 1 - (np.mean(devs) / (tol*glob_mean)))"
   ],
   "id": "2273b32913affc98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_convergence_rate(paths, true_val):\n",
    "    cum_means = np.cumsum(paths)/np.arange(1, len(paths)+1)\n",
    "    difs = np.abs(cum_means - true_val)\n",
    "    final = difs[-1]\n",
    "    avg = np.mean(difs)\n",
    "    if avg == 0.0:\n",
    "        return 1.0\n",
    "    return max(0, 1-(final/avg))"
   ],
   "id": "b45e0e8c02d20d90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def rate_mc_simulation(paths, true_value, runtime, weights):\n",
    "    mean_estimate = np.mean(paths)\n",
    "    variance = np.var(paths)\n",
    "    bias = mean_estimate - np.mean(true_value)\n",
    "    mse = bias**2 + variance\n",
    "    efficiency = 1 / (runtime * mse)\n",
    "\n",
    "    robustness = test_robustness(paths.mean(axis=1), 150)\n",
    "\n",
    "    convergence_rate = compute_convergence_rate(paths.mean(axis=1), true_value)\n",
    "\n",
    "    score = (\n",
    "            weights[0] * convergence_rate +\n",
    "            weights[1] * (1 / np.std(paths)) +\n",
    "            weights[2] * (1 / abs(bias)) +\n",
    "            weights[3] * (1 / variance) +\n",
    "            weights[4] * efficiency +\n",
    "            weights[5] * robustness)\n",
    "    return score"
   ],
   "id": "35ee8fe1e1d2c6df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "weights=[1/6,1/6,1/6,1/6,1/6,1/6]\n",
    "pd.Series(pp['EWSA.AS'].mean(axis=1))"
   ],
   "id": "9b9f205980a01720"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print((rate_mc_simulation(pp['EWSA.AS'], funds_w_names[\"EWSA.AS\"][\"Close\"], 0.1, weights)))",
   "id": "cfdb745776196b68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "REDESIGN PORTFOLIO",
   "id": "3935f609281043cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "list of predictions for each fund in portfolio\n",
    "'''\n",
    "portfolio = funds_for_hedge\n",
    "preds_for_portfolio = []\n",
    "for i in portfolio:\n",
    "    preds_for_portfolio.append(fit_predict(pred_len=29, epochs=200, batch_size=128, lr=1e-3, dropout=0.1, df=funds_w_names[i], independent_variables=[\"Open\", \"High\", \"Low\", \"isOverbought\", \"Bullish\"]))"
   ],
   "id": "4cd92b7253b1325e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "calculate ratio of a single fund based on provided predictions\n",
    "'''\n",
    "def calculate_funds_ratio(funds_preds, fund: pd.DataFrame):\n",
    "    funds_return = fund[\"Close\"].iloc[-1] - funds_preds[\"Close\"].iloc[-1]\n",
    "    funds_risk = volatility_mc(monte_carlo_sim(fund))\n",
    "    return funds_return/funds_risk, funds_return"
   ],
   "id": "ce2cdb4957d7173"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scipy.optimize import minimize\n",
    "def redistribute_funds(funds_dfs:list, weights: list):\n",
    "    \"\"\"\n",
    "    for a given portfolio, calculate risk and return for next month for each asset and decide whether to buy more of it\n",
    "    funds_dfs is a list of funds dataframes based on portfolio\n",
    "    weights is a list of weights for each funds\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(w):\n",
    "        portfolio_score = 0\n",
    "        for i in (range(len(funds_dfs))):\n",
    "            ratio, returnn, preds = calculate_funds_ratio(preds_for_portfolio, funds_dfs[i])\n",
    "            portfolio_score += ratio * w[i]\n",
    "            return -portfolio_score\n",
    "\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    bounds = [(0,1)] * len(funds_dfs)\n",
    "    result = minimize(objective, np.ndarray(weights), method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    optimized_weights = result.x\n",
    "    optimized_score = result.fun\n",
    "\n",
    "    returns = [calculate_funds_ratio(preds_for_portfolio[i], funds_dfs[i])[1] for i in range(len(portfolio))]\n",
    "    final_gain = optimized_weights*returns\n",
    "\n",
    "    return optimized_weights, optimized_score, final_gain"
   ],
   "id": "172e140791bcff80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "works for small Ns, max 10",
   "id": "77b73e521e394387"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def design_optimal_portfolio(N: int):\n",
    "    available_funds = list(funds_w_names.keys())\n",
    "    fund_combinations = list(combinations(available_funds, N))\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_weights = None\n",
    "    best_selected_funds = None\n",
    "    best_final_gains = None\n",
    "\n",
    "    for selected_funds in fund_combinations:\n",
    "\n",
    "        funds_dfs = [funds_w_names[fund] for fund in selected_funds]\n",
    "\n",
    "\n",
    "        preds_for_selected_funds = []\n",
    "        for fund in selected_funds:\n",
    "            preds_for_selected_funds.append(fit_predict(\n",
    "                pred_len=29, epochs=200, batch_size=128, lr=1e-3, dropout=0.1, df=funds_w_names[fund],\n",
    "                independent_variables=[\"Open\", \"High\", \"Low\", \"isOverbought\", \"Bullish\"]\n",
    "            ))\n",
    "\n",
    "\n",
    "        def objective(w):\n",
    "            portfolio_score = 0\n",
    "            for i in range(len(funds_dfs)):\n",
    "                ratio, returnn, _ = calculate_funds_ratio(preds_for_selected_funds[i], funds_dfs[i])\n",
    "                portfolio_score += ratio * w[i]\n",
    "            return -portfolio_score\n",
    "\n",
    "\n",
    "        constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "        bounds = [(0, 1)] * N\n",
    "\n",
    "        initial_weights = np.ones(N) / N\n",
    "\n",
    "        result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "        optimized_weights = result.x\n",
    "        optimized_score = -result.fun\n",
    "\n",
    "        returns = [calculate_funds_ratio(preds_for_selected_funds[i], funds_dfs[i])[1] for i in range(N)]\n",
    "        final_gains = np.dot(optimized_weights, returns)\n",
    "\n",
    "        if optimized_score > best_score:\n",
    "            best_score = optimized_score\n",
    "            best_weights = optimized_weights\n",
    "            best_selected_funds = selected_funds\n",
    "            best_final_gains = final_gains\n",
    "\n",
    "    return best_weights, best_score, best_final_gains, best_selected_funds"
   ],
   "id": "21899c9b0087c8ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "N = 5\n",
    "best_weights, best_score, best_final_gains, best_selected_funds = design_optimal_portfolio(N)\n",
    "\n",
    "print(\"Selected funds:\", best_selected_funds)\n",
    "print(\"Optimized weights:\", best_weights)\n",
    "print(\"Best gain:\", best_final_gains)"
   ],
   "id": "b309d122088a8ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "works for large Ns",
   "id": "f9b3258ad91e78dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "from scipy.optimize import minimize\n",
    "from itertools import combinations\n",
    "\n",
    "def calculate_fitness(preds_for_selected_funds, funds_dfs, weights):\n",
    "    portfolio_score = 0\n",
    "    for i in range(len(funds_dfs)):\n",
    "        ratio, returnn, _ = calculate_funds_ratio(preds_for_selected_funds[i], funds_dfs[i])\n",
    "        portfolio_score += ratio * weights[i]\n",
    "    return portfolio_score\n",
    "\n",
    "def genetic_algorithm(funds_w_names, N, population_size=50, generations=100, mutation_rate=0.1, crossover_rate=0.8):\n",
    "    available_funds = list(funds_w_names.keys())\n",
    "    fund_combinations = list(combinations(available_funds, N))\n",
    "\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        selected_funds = random.sample(fund_combinations, 1)[0]\n",
    "        weights = np.random.rand(N)\n",
    "        weights /= np.sum(weights)\n",
    "        population.append((selected_funds, weights))\n",
    "\n",
    "    best_solution = None\n",
    "    best_fitness = -np.inf\n",
    "\n",
    "    for generation in range(generations):\n",
    "        fitness_scores = []\n",
    "        preds_for_selected_funds_list = []\n",
    "        funds_dfs_list = []\n",
    "\n",
    "        for selected_funds, _ in population:\n",
    "            funds_dfs = [funds_w_names[fund] for fund in selected_funds]\n",
    "            preds_for_selected_funds = []\n",
    "            for fund in selected_funds:\n",
    "                preds_for_selected_funds.append(fit_predict(\n",
    "                    pred_len=29, epochs=200, batch_size=128, lr=1e-3, dropout=0.1,\n",
    "                    df=funds_w_names[fund], independent_variables=[\"Open\", \"High\", \"Low\", \"isOverbought\", \"Bullish\"]\n",
    "                ))\n",
    "            preds_for_selected_funds_list.append(preds_for_selected_funds)\n",
    "            funds_dfs_list.append(funds_dfs)\n",
    "\n",
    "        for i in range(population_size):\n",
    "            selected_funds, weights = population[i]\n",
    "            fitness = calculate_fitness(preds_for_selected_funds_list[i], funds_dfs_list[i], weights)\n",
    "            fitness_scores.append(fitness)\n",
    "            if fitness > best_fitness:\n",
    "                best_fitness = fitness\n",
    "                best_solution = (selected_funds, weights)\n",
    "\n",
    "        selected_parents = []\n",
    "        fitness_sum = np.sum(fitness_scores)\n",
    "        prob = [score / fitness_sum for score in fitness_scores]\n",
    "\n",
    "        for _ in range(population_size):\n",
    "            selected_parent = random.choices(population, prob)[0]\n",
    "            selected_parents.append(selected_parent)\n",
    "\n",
    "        new_population = []\n",
    "        for i in range(0, population_size, 2):\n",
    "            parent1, parent2 = selected_parents[i], selected_parents[i+1]\n",
    "            if random.random() < crossover_rate:\n",
    "                crossover_point = random.randint(1, N-1)  # Crossover at a random point\n",
    "                offspring1 = (parent1[0][:crossover_point] + parent2[0][crossover_point:],\n",
    "                              np.concatenate((parent1[1][:crossover_point], parent2[1][crossover_point:])))\n",
    "                offspring2 = (parent2[0][:crossover_point] + parent1[0][crossover_point:],\n",
    "                              np.concatenate((parent2[1][:crossover_point], parent1[1][crossover_point:])))\n",
    "                new_population.extend([offspring1, offspring2])\n",
    "            else:\n",
    "                new_population.extend([parent1, parent2])\n",
    "\n",
    "        for i in range(population_size):\n",
    "            if random.random() < mutation_rate:\n",
    "                selected_funds, weights = new_population[i]\n",
    "                mutation_idx = random.randint(0, N-1)\n",
    "                weights[mutation_idx] = random.random()\n",
    "                weights /= np.sum(weights)  # Normalize weights to sum to 1\n",
    "                new_population[i] = (selected_funds, weights)\n",
    "\n",
    "        population = new_population\n",
    "\n",
    "    return best_solution, best_fitness"
   ],
   "id": "8a83a009ecdbff5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "N = 5\n",
    "best_solution, best_fitness = genetic_algorithm(funds_w_names, N)\n",
    "\n",
    "selected_funds, optimized_weights = best_solution\n",
    "print(\"Selected funds:\", selected_funds)\n",
    "print(\"Optimized weights:\", optimized_weights)\n",
    "print(\"Best fitness (risk-adjusted return):\", best_fitness)"
   ],
   "id": "ee0a6a49f8fa7470"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
