{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import datetime\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from tkinter.constants import FIRST\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import stock_indicators as sa\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import pandas as pd\n",
    "from numpy.core.records import ndarray\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "import yfinance as yf\n",
    "from stock_indicators import PeriodSize, Quote\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from stock_indicators.indicators.common.enums import PeriodSize\n",
    "from sympy.strategies import minimize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "from tqdm.autonotebook import tqdm\n",
    "from cvxopt import matrix, solvers"
   ],
   "id": "6ec948fcbd2c8c5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(0)"
   ],
   "id": "40f3232802bf328f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "funds_w_names = defaultdict(pd.DataFrame)",
   "id": "aa969f7bfddc7a22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "etf_names = ['IE0005042456', 'EDMU.SW', 'EDG2.L', '36BA.DE', 'CBUS.DE', 'EWSA.AS','IE00B0M62X26','IE00B14X4Q57','IE00B1XNHC34','IE00B3FH7618','IE00B3ZW0K18','IE00B52MJY50','IE00B5M4WH52','IE00B66F4759','EMBE.L', 'DTLE.L','IE00BDFK1573','IBC5.DE','IE00BFNM3G45','IE00BHZPJ015','IE00BHZPJ452','IE00BHZPJ783',\n",
    "'IE00BLDGH553','IE00BMG6Z448','IE00BYYHSM20','IE00BYZTVT56','IE00BZ173V67','IE00BZ1NCS44','ISVIF', \"IESE.AS\", \"UEEF.DE\"]"
   ],
   "id": "59d2a64ac6eb5533"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(etf_names)",
   "id": "6e0a799c10f8e9d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for fund in etf_names:\n",
    "    print(fund)\n",
    "    a = (pd.DataFrame(yf.Ticker(fund).history(start=\"2023-01-03\", end=\"2024-01-01\")))\n",
    "    a.index = pd.to_datetime(a.index)\n",
    "    a.index = a.index.normalize()\n",
    "    full_date_range = pd.date_range(start=a.index.min(), end=a.index.max(), freq=\"D\")\n",
    "    a = a.reindex(full_date_range)\n",
    "\n",
    "    a[\"Return\"] = a[\"Close\"].pct_change()\n",
    "    a[\"Rolling Volatility\"] = a[\"Return\"].rolling(7).std()\n",
    "\n",
    "    a.fillna(method=\"bfill\", inplace=True)\n",
    "    if fund != 'ISVIF':\n",
    "        a.drop([\"Dividends\", \"Stock Splits\", \"Capital Gains\"], inplace=True, axis=1)\n",
    "    else:\n",
    "        a.drop([\"Dividends\", \"Stock Splits\"], inplace=True, axis=1)\n",
    "    funds_w_names[fund] = a"
   ],
   "id": "e5657d6db7f1b677"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import pickle\n",
    "# pickle.dump(funds_w_names, open('funds_w_names.pkl', 'wb'))"
   ],
   "id": "6173476cff2c5e25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import pickle\n",
    "# pickle.dump(funds_w_names, open('funds_w_names_2.pkl', 'wb'))"
   ],
   "id": "5f62da495c1d029c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def portfolio_factory(etfs: list):\n",
    "    giga_fund = pd.DataFrame()\n",
    "    for c, ff in enumerate(etfs):\n",
    "        f = ff.copy()\n",
    "        # Ensure index is datetime\n",
    "        f.index = pd.to_datetime(f.index, utc=True)\n",
    "        if f.index.hour[0]==0:\n",
    "            f.index = f.index+datetime.timedelta(hours=-1)\n",
    "        f.index = f.index.normalize()\n",
    "\n",
    "        # Sort by datetime index (important for time series models)\n",
    "        f = f.sort_index()\n",
    "\n",
    "        # Add time_idx relative to the start of the individual ETF\n",
    "        f[\"time_idx\"] = (f.index - f.index.min()).days\n",
    "\n",
    "        # Add unique group_id\n",
    "        f[\"group_id\"] = c\n",
    "\n",
    "        # Reset index to avoid issues with overlapping indices\n",
    "        f = f.reset_index()\n",
    "\n",
    "        # Concatenate into the master DataFrame\n",
    "        giga_fund = pd.concat([giga_fund, f], ignore_index=True)\n",
    "    return giga_fund"
   ],
   "id": "11e7f2f54c1b6c59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pd.options.display.date_yearfirst = True  # Ensures year-first format\n",
    "pd.options.display.float_format = '{:.2f}'.format  # Example for float formatting"
   ],
   "id": "3e01cad0be0e66e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def technical_indicators_factory(etfs: list, names: list):\n",
    "    indicators_per_fund = defaultdict(pd.DataFrame)\n",
    "    figs = []\n",
    "\n",
    "    for id, f in enumerate(etfs):\n",
    "        quotes_for_f = [\n",
    "            Quote(\n",
    "                date=row.Index,\n",
    "                open=row.Open,\n",
    "                high=row.High,\n",
    "                low=row.Low,\n",
    "                close=row.Close,\n",
    "                volume=row.Volume\n",
    "            )\n",
    "            for row in f.itertuples()]\n",
    "\n",
    "\n",
    "        macd_for_f = sa.indicators.get_macd(quotes=quotes_for_f)\n",
    "        valid_macd_for_f = [\n",
    "            (result.date, result.macd, result.signal, result.histogram)\n",
    "            for result in macd_for_f\n",
    "            if result.macd is not None and result.signal is not None and result.histogram is not None\n",
    "        ]\n",
    "        macd_dates, macd_values, macd_signal, macd_histogram = zip(*valid_macd_for_f)\n",
    "\n",
    "\n",
    "        rsi_for_f = sa.indicators.get_rsi(quotes=quotes_for_f)\n",
    "        valid_rsi_for_f = [\n",
    "            (result.date, result.rsi)\n",
    "            for result in rsi_for_f\n",
    "            if result.date is not None and result.rsi is not None\n",
    "        ]\n",
    "        rsi_dates, rsi_values = zip(*valid_rsi_for_f)\n",
    "\n",
    "\n",
    "        bb_for_f = sa.indicators.get_bollinger_bands(quotes=quotes_for_f)\n",
    "        valid_bb_for_f = [\n",
    "            (result.date, result.lower_band, result.upper_band)\n",
    "            for result in bb_for_f\n",
    "            if result.date is not None and result.lower_band is not None\n",
    "        ]\n",
    "        bb_dates, bb_lower, bb_upper = zip(*valid_bb_for_f)\n",
    "\n",
    "\n",
    "        vwap_for_f = sa.indicators.get_vwap(quotes=quotes_for_f)\n",
    "        valid_vwap_for_f = [\n",
    "            (result.date, result.vwap)\n",
    "            for result in vwap_for_f\n",
    "            if result.date is not None and result.vwap is not None\n",
    "        ]\n",
    "        vwap_dates, vwap_values = zip(*valid_vwap_for_f)\n",
    "\n",
    "\n",
    "\n",
    "        pp_for_f = sa.indicators.get_pivot_points(quotes=quotes_for_f, window_size=PeriodSize.DAY)\n",
    "        valid_pp_for_f = [\n",
    "            (result.date, result.pp, result.r1, result.r2, result.r3, result.r4, result.s1, result.s2, result.s3, result.s4)\n",
    "            for result in pp_for_f\n",
    "            if result.date is not None and result.pp is not None\n",
    "        ]\n",
    "        pp_dates, pp, r1, r2, r3, r4, s1, s2, s3, s4 = zip(*valid_pp_for_f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        fig, axes = plt.subplots(5, 1, figsize=(12,6))\n",
    "\n",
    "\n",
    "        axes[0].plot(macd_dates[-20:], macd_values[-20:], label='MACD_values', linewidth=2)\n",
    "        axes[0].plot(macd_dates[-20:], macd_signal[-20:], label='MACD_signal', linewidth=2)\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        axes[1].plot(rsi_dates[-20:], rsi_values[-20:], label='RSI', linewidth=2)\n",
    "        axes[1].legend(loc='best')\n",
    "\n",
    "        axes[2].plot(bb_dates[-20:], bb_lower[-20:], label='BB_lower', linewidth=2)\n",
    "        axes[2].plot(bb_dates[-20:], bb_upper[-20:], label='BB_upper', linewidth=2)\n",
    "        axes[2].legend(loc='best')\n",
    "\n",
    "        axes[3].plot(vwap_dates[-20:], vwap_values[-20:], label='vwap', linewidth=2)\n",
    "        axes[3].legend(loc='best')\n",
    "\n",
    "        axes[4].plot(pp_dates[-20:], pp[-20:], label='pivot points', linewidth=2)\n",
    "        axes[4].plot(pp_dates[-20:], r1[-20:], label='r1', linewidth=2)\n",
    "        axes[4].plot(pp_dates[-20:], r4[-20:], label='r4', linewidth=2)\n",
    "        axes[4].plot(pp_dates[-20:], s1[-20:], label='s1', linewidth=2)\n",
    "        axes[4].plot(pp_dates[-20:], s4[-20:], label='s4', linewidth=2)\n",
    "        axes[4].legend(loc='best')\n",
    "\n",
    "\n",
    "        figs.append(fig)\n",
    "\n",
    "        f['Bullish'] = [1 if x.histogram and x.histogram > 0 else 0 for x in macd_for_f]\n",
    "        f['Bearish'] = [1 if x.histogram and x.histogram < 0 else 0 for x in macd_for_f]\n",
    "        f['isOverbought'] = [1 if x.rsi and x.rsi > 70 else 0 for x in rsi_for_f ]\n",
    "        f['isOversold'] = [1 if x.rsi and x.rsi < 30 else 0 for x in rsi_for_f ]\n",
    "\n",
    "\n",
    "        f.fillna(method=\"ffill\", inplace=True)\n",
    "        indicators_per_fund[names[id]]= f\n",
    "    return indicators_per_fund, figs"
   ],
   "id": "2f3e32cc570abe3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def fit_predict(pred_len, epochs: int, batch_size: int, lr: float, dropout: float, df: pd.DataFrame, independent_variables: list, target=\"Close\", training_cutoff_idx: pd.Timestamp=333):\n",
    "    training = TimeSeriesDataSet(\n",
    "        df[lambda x: x.time_idx <= training_cutoff_idx],  # Use the determined cutoff index\n",
    "        time_idx=\"time_idx\",                              # Sequential time index\n",
    "        target=target,                                    # Target variable\n",
    "        group_ids=[\"group_id\"],                           # Group identifier\n",
    "        min_encoder_length=22,                             # Minimum input sequence length\n",
    "        max_encoder_length=365,                            # Maximum input sequence length\n",
    "        min_prediction_length=1,                         # Minimum forecast length\n",
    "        max_prediction_length=pred_len,                   # Maximum forecast length (31 for December)\n",
    "        static_reals=[],                                  # No static real variables\n",
    "        time_varying_known_reals= independent_variables,  # Known inputs\n",
    "        time_varying_unknown_reals=[target],              # Target variable\n",
    "        target_normalizer=GroupNormalizer(groups=[\"group_id\"], transformation=\"softplus\"),\n",
    "        add_relative_time_idx=True,                       # Add relative time index\n",
    "        add_target_scales=True,                           # Scale the target variable\n",
    "        add_encoder_length=True,                          # Include encoder length feature\n",
    "        allow_missing_timesteps=False,                    # Ensure no missing steps\n",
    "    )\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(\n",
    "        training,\n",
    "        df,\n",
    "        predict=True,\n",
    "        stop_randomization=True                             # Ensures no randomization in validation dataset\n",
    "    )\n",
    "\n",
    "    batch_size = batch_size\n",
    "\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=lr,              # Learning rate\n",
    "        hidden_size=16,                  # Model hidden size\n",
    "        attention_head_size=1,           # Number of attention heads\n",
    "        dropout=dropout,                     # Dropout rate\n",
    "        hidden_continuous_size=8,        # Hidden size for continuous variables\n",
    "        output_size=7,                   # Output quantiles (e.g., 10th to 90th percentile)\n",
    "        loss=QuantileLoss(),             # Loss function\n",
    "        log_interval=10,                 # Log every 10 batches\n",
    "        reduce_on_plateau_patience=4,    # Reduce learning rate on plateau\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints/\",\n",
    "        filename=\"tft-model-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\",  #\n",
    "        mode=\"min\",  # minimize validation loss\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=8, verbose=False, mode=\"min\")\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        # logger=logger,\n",
    "        max_epochs=epochs,\n",
    "        accelerator='cuda',\n",
    "        devices=\"auto\",\n",
    "        gradient_clip_val=0.1,\n",
    "        callbacks=[early_stop_callback, lr_logger],#, checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    trainer.fit(tft, train_dataloader, val_dataloader)\n",
    "    raw_predictions = tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    predicted_median_np = raw_predictions.output.prediction[0, :, 1].detach().cpu().numpy()\n",
    "    return predicted_median_np"
   ],
   "id": "36430f7fe024eb09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def metrics_and_plt(df: pd.DataFrame, preds:list, date_range:pd.date_range, target=\"Close\", date = \"2023-12-01\"):\n",
    "    actual_values = df[target][date:]\n",
    "    mse = mean_squared_error(actual_values, preds)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(date_range, actual_values, marker=\"x\", label=\"Actual Close\", color=\"orange\")\n",
    "    plt.plot(date_range, preds, marker=\"o\", label=\"Predicted Median\")\n",
    "    plt.title(\"Predicted Values for December 01 to December 29, 2023\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Predicted Value\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return mse, plt"
   ],
   "id": "b65a4ca80bad99bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "NO INDICATORS, ONE ETF",
   "id": "78c7fc70a8e598f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#IE00B52MJY50\n",
    "IE = portfolio_factory(etfs=[funds_w_names['EWSA.AS']])"
   ],
   "id": "b8ac824d8a9c84b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE",
   "id": "cf99937cb134f1f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_preds = fit_predict(pred_len=29, epochs=100, batch_size=128, lr=1e-3, dropout=0.1, df=IE, independent_variables=[\"Open\", \"High\", \"Low\", \"Volume\"])",
   "id": "6b493194cc657e50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_mse, IE_plt = metrics_and_plt(IE, IE_preds, pd.date_range(start=\"2023-12-01\", end=\"2023-12-29\"))",
   "id": "345541cb30152943"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_mse",
   "id": "b1226ee5d8b12063"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ADDING INDICATORS",
   "id": "494b056d418cf555"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_indicators, IE_indicators_plt1 = technical_indicators_factory([funds_w_names['EWSA.AS']], ['EWSA.AS'])",
   "id": "ef28da943fc46deb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "analiza = IE_indicators[\"EWSA.AS\"][[\"isOversold\", \"isOverbought\", \"Bullish\", \"Bearish\", \"Return\"]]",
   "id": "c18373332e3e2955"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "corr1 = analiza.corr()\n",
    "sns.heatmap(corr1, annot=True, cmap=\"coolwarm\", fmt=\".2f\")"
   ],
   "id": "1edbdc487987dcfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "correlations = IE_indicators[\"EWSA.AS\"].corr()",
   "id": "275c48a55af89658"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(correlations, annot=True, cmap=\"coolwarm\", fmt=\".2f\")"
   ],
   "id": "5f33a4af4efc5dd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_indicators_preds = fit_predict(pred_len=29, epochs=220, batch_size=128, lr=1e-3, dropout=0.1, df=IE_indicators[\"EWSA.AS\"], independent_variables=[\"Open\", \"High\", \"Low\", \"isOverbought\", \"Bullish\"])",
   "id": "c888f6941f8e9b3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_indicators_mse, IE_indicators_plt2= metrics_and_plt(IE, IE_indicators_preds, pd.date_range(start=\"2023-12-01\", end=\"2023-12-29\"))",
   "id": "5e513643dc478b35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_indicators_mse",
   "id": "f70d1d38302c9027"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ENSAMBLE MODEL 2: REMAINDERS",
   "id": "4d04bc8b20a04f52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "resztki = abs(IE_indicators_preds  - IE_indicators[\"EWSA.AS\"][\"Close\"][\"2023-12-01\":])",
   "id": "3e179ec29d7738d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "resztki",
   "id": "1922c9a3ae91a380"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_scary = IE_indicators[\"EWSA.AS\"][333:].copy()\n",
    "df_scary[\"Remainders\"] = resztki"
   ],
   "id": "99e82d8430b364ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_scary",
   "id": "57a0515d8d9d5fd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_scary.index = pd.to_datetime(df_scary.index)  # ensure index is datetime if not already\n",
    "df_scary[\"time_idx\"] = (df_scary.index.date - df_scary.index.date[0]).astype(\"timedelta64[D]\").astype(int)\n",
    "df_scary['group_id'] = 0"
   ],
   "id": "df55aad045f04009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_model2 = fit_predict(pred_len=7,epochs=220, batch_size=128, lr=1e-3, dropout=0.1, df=df_scary, independent_variables=[\"Open\", \"High\", \"Low\", \"isOverbought\", \"Bullish\"], target=\"Remainders\")",
   "id": "b74724b451bd3761"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_model2_mse, IE_model2_plt= metrics_and_plt(df_scary, IE_model2, pd.date_range(start=\"2023-12-23\", end=\"2023-12-29\"), target=\"Remainders\", date=\"2023-12-23\")",
   "id": "3091583c4eb45c9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_preds = IE_indicators_preds.copy()[-7:] + IE_model2",
   "id": "1eeb40d7d27dcb12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_mse = mean_squared_error(IE_indicators[\"EWSA.AS\"][\"Close\"][-7:], final_preds)",
   "id": "fa7b8ccafee6c367"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_mse",
   "id": "3865380aea58b0eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "FULL HEDGE PORTFOLIO",
   "id": "88d9b758aba9e7be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "funds_for_hedge = [\"ISVIF\", \"IE00BHZPJ783\", \"36BA.DE\", \"IE00B3ZW0K18\", \"IE00BMG6Z448\"]",
   "id": "b286599280147e64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "funds_with_indics = []\n",
    "for fund in funds_for_hedge:\n",
    "    funds_with_indics.append(funds_w_names[fund].copy())"
   ],
   "id": "a06708e0f9d996ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "fund_name_fund_df, sum_plt = technical_indicators_factory(funds_with_indics, funds_for_hedge)",
   "id": "18ab54051bff95c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "GIGAFUND = portfolio_factory(etfs=[fund_name_fund_df[k] for k in (funds_for_hedge)])",
   "id": "9af320091777c15e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "GIGAFUND_NORM = GIGAFUND.copy()\n",
    "\n",
    "columns_to_normalize = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Return\", \"Rolling Volatility\"]\n",
    "\n",
    "GIGAFUND_NORM[columns_to_normalize] = (\n",
    "    GIGAFUND_NORM[columns_to_normalize]\n",
    "    .apply(lambda col: (col - col.min()) / (col.max() - col.min()), axis=0)\n",
    ")"
   ],
   "id": "719abeb814db09c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "GIGAFUND_NORM",
   "id": "181c672544118af4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "duplicate_rows = GIGAFUND_NORM[GIGAFUND_NORM.duplicated(subset=[\"time_idx\", \"group_id\"], keep=False)]\n",
    "print(duplicate_rows)"
   ],
   "id": "acba55612fa5307f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "IE_indicators_preds = fit_predict(pred_len=29, epochs=220, batch_size=128, lr=1e-3, dropout=0.1, df=GIGAFUND_NORM, independent_variables=[\"Open\", \"High\", \"Low\", \"isOverbought\", \"Bullish\"])",
   "id": "c0d3ea0ec55495f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "RISK: Monte Carlo and Transformer Predicting VARIANCE",
   "id": "599d3dd2f8f53a2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calc_drift(funds):\n",
    "    drift_dict={}\n",
    "    for k, v in funds.items():\n",
    "        log_returns = np.log(1+funds[k][\"Close\"].pct_change())\n",
    "        log_returns.fillna(value=0, inplace=True)\n",
    "        avg_pdr = log_returns.mean()\n",
    "        var = log_returns.var()\n",
    "        drift = avg_pdr-(.5*var)\n",
    "        drift_dict[k]=drift\n",
    "    return drift_dict"
   ],
   "id": "892be6565a7a4652"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def monte_carlo_sim(funds, drifts):\n",
    "    price_paths={}\n",
    "    for k, v in funds.items():\n",
    "        log_returns = np.log(1+funds[k][\"Close\"].pct_change())\n",
    "        log_returns.fillna(value=0, inplace=True)\n",
    "        stdev=log_returns.std()\n",
    "        days=362\n",
    "        trials=100\n",
    "        Z = norm.ppf(np.random.rand(days,trials))\n",
    "        daily_returns=np.exp(np.array(drifts[k]) + np.array(stdev) * Z)\n",
    "        price_paths[k] = np.zeros_like(daily_returns)\n",
    "        price_paths[k][0] = funds[k][\"Close\"].iloc[-1]\n",
    "        for i in range(1,days):\n",
    "            price_paths[k][i] = price_paths[k][i-1] * daily_returns[i-1]\n",
    "    return price_paths"
   ],
   "id": "3558d523bcfa9938"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "drifts=calc_drift(funds_w_names)\n",
    "pp=monte_carlo_sim(funds_w_names, drifts=drifts)"
   ],
   "id": "c5a64c51b1c8336a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pp['EWSA.AS']",
   "id": "26231ee0195a6f25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pps= pd.DataFrame(pp['EWSA.AS'])\n",
    "pps = pps.pct_change()\n",
    "pps.fillna(value=0, inplace=True)\n",
    "pps= pps.rolling(7).std()\n",
    "pps.fillna(method=\"bfill\", inplace=True)\n",
    "pps"
   ],
   "id": "9bc788dee2f35513"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "EWSA_volatility = pd.DataFrame(pps)\n",
    "EWSA_volatility= EWSA_volatility.mean(axis=1)"
   ],
   "id": "e15a398052cf897f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "EWSA_volatility",
   "id": "975162e65676bf92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pps",
   "id": "748c969e0ec7b10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(pp[\"EWSA.AS\"])"
   ],
   "id": "62de0a2e16e2dd4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ed45beead402d07b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def nearest_positive_definite(matrix):\n",
    "    P = matrix.copy()\n",
    "    eigvals, eigvecs = np.linalg.eigh(P)\n",
    "    eigvals[eigvals < 0] = 1e-10\n",
    "    return eigvecs @ np.diag(eigvals) @ eigvecs.T"
   ],
   "id": "c4dd82f28cc0681f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "variances = {}\n",
    "covs_means = []\n",
    "for k,v in funds_w_names.items():\n",
    "    variances[k] = v[\"Rolling Volatility\"]\n",
    "variances = pd.DataFrame(variances)\n",
    "r = variances.pct_change()\n",
    "covariance_matrix = r.cov()\n",
    "mean_returns = r.mean()"
   ],
   "id": "2d8040f3f142c848"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not np.all(np.linalg.eigvals(covariance_matrix) > 0):\n",
    "    print(\"Covariance matrix not positive definite. Adding regularization.\")\n",
    "    covariance_matrix = nearest_positive_definite(covariance_matrix)"
   ],
   "id": "b874eeeb3e8274ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "covariance_matrix",
   "id": "9f60e461755cc310"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#to be filled with real data\n",
    "portfolio_weights = np.random.random(len(mean_returns)) # distribution of etfs in portfolio\n",
    "portfolio_weights /= np.sum(portfolio_weights)"
   ],
   "id": "2e249730f3e2201f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "initial_portfoloio = 0\n",
    "for i, v in enumerate(funds_w_names.values()):\n",
    "    initial_portfoloio += portfolio_weights[i] * v[\"Close\"][i]"
   ],
   "id": "a9e0e1fa06653206"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "days = 60\n",
    "simulations = 100\n",
    "portfolio_sims = np.full(shape=(days, simulations), fill_value=0.0) # default\n",
    "mean_matrix = np.full(shape=(days, len(portfolio_weights)), fill_value=mean_returns).T # days x all etfs"
   ],
   "id": "2c45a12ddaf7b2fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "initial_portfoloio",
   "id": "15b60ab029ac1df5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for s in range(0, simulations):\n",
    "    Z = np.random.normal(size=(days, len(portfolio_weights)))\n",
    "    L = np.linalg.cholesky(covariance_matrix)\n",
    "    daily_returns = mean_matrix + np.inner(L, Z)\n",
    "    portfolio_sims[:, s] = np.cumprod(np.inner(portfolio_weights, daily_returns.T)+1)*initial_portfoloio\n",
    "\n",
    "plt.plot(portfolio_sims)\n",
    "plt.ylabel(\"Portfolio Simulations\")"
   ],
   "id": "3e26aa1800a6e5ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "discounted_prices = [sim[-1]*0.96 for sim in portfolio_sims]",
   "id": "a56752e45c807c13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "avg = np.average(discounted_prices)",
   "id": "4ef2f23daa86fc14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "avg",
   "id": "fb2c1936da52ce7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MONTE CARLO EVALUATION FUNCITON",
   "id": "fe80c0f5630fd8d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test_robustness(paths, num_trials, tol=0.01):\n",
    "    means = []\n",
    "    for i in range(num_trials):\n",
    "        np.random.seed(i)\n",
    "        new_paths = np.random.choice(paths, size=len(paths), replace=True)\n",
    "        means.append(np.mean(new_paths))\n",
    "    glob_mean = np.mean(means)\n",
    "    devs = np.abs(np.array(means) - glob_mean)\n",
    "    return max(0, 1 - (np.mean(devs) / (tol*glob_mean)))"
   ],
   "id": "8e5ed7cda9eaf42f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_convergence_rate(paths, true_val):\n",
    "    cum_means = np.cumsum(paths)/np.arange(1, len(paths)+1)\n",
    "    difs = np.abs(cum_means - true_val)\n",
    "    final = difs[-1]\n",
    "    avg = np.mean(difs)\n",
    "    if avg == 0.0:\n",
    "        return 1.0\n",
    "    return max(0, 1-(final/avg))"
   ],
   "id": "76ab077726719f96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def rate_mc_simulation(paths, true_value, runtime, weights):\n",
    "    mean_estimate = np.mean(paths)\n",
    "    variance = np.var(paths)\n",
    "    bias = mean_estimate - np.mean(true_value)\n",
    "    mse = bias**2 + variance\n",
    "    efficiency = 1 / (runtime * mse)\n",
    "\n",
    "    robustness = test_robustness(paths.mean(axis=1), 150)\n",
    "\n",
    "    convergence_rate = compute_convergence_rate(paths.mean(axis=1), true_value)\n",
    "\n",
    "    score = (\n",
    "            weights[0] * convergence_rate +\n",
    "            weights[1] * (1 / np.std(paths)) +  # Inverse of variability\n",
    "            weights[2] * (1 / abs(bias)) +\n",
    "            weights[3] * (1 / variance) +\n",
    "            weights[4] * efficiency +\n",
    "            weights[5] * robustness)\n",
    "    return score"
   ],
   "id": "e51b9d2204d94fa6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "weights=[1/6,1/6,1/6,1/6,1/6,1/6]\n",
    "pd.Series(pp['EWSA.AS'].mean(axis=1))"
   ],
   "id": "f456dc36d71416da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print((rate_mc_simulation(pp['EWSA.AS'], funds_w_names[\"EWSA.AS\"][\"Close\"], 0.1, weights)))",
   "id": "73594c0d071e139e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "REDESIGN PORTFOLIO",
   "id": "7d0580164ca9192d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_funds_ratio(fund: pd.DataFrame):\n",
    "    funds_preds = fit_predict(pred_len=29, epochs=200, batch_size=128, lr=1e-3, dropout=0.1, df=fund, independent_variables=[\"Open\", \"High\", \"Low\", \"isOverbought\", \"Bullish\"])\n",
    "    funds_return = fund[\"Close\"].iloc[-1] - funds_preds[\"Close\"].iloc[-1]\n",
    "    funds_risk = 0\n",
    "    return funds_preds/funds_return"
   ],
   "id": "6b9d4b6bee945080"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scipy.optimize import minimize\n",
    "def redistribute_funds(funds_dfs:list, weights: list):\n",
    "    \"\"\"\n",
    "    for a given portfolio, calculate risk and return for next month for each asset and decide whether to buy more of it\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(w):\n",
    "        portfolio_score = 0\n",
    "        for i in (range(len(funds_dfs))):\n",
    "            fund_ratio = calculate_funds_ratio(funds_dfs[i])\n",
    "            portfolio_score += fund_ratio * w[i]\n",
    "            return -portfolio_score\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    bounds = [(0,1)] * len(funds_dfs)\n",
    "    result = minimize(objective, ndarray(weights), method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    optimized_weights = result.x\n",
    "    optimized_score = result.fun\n",
    "\n",
    "    return optimized_weights, optimized_score"
   ],
   "id": "3bbc76519dce48a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
