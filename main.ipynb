{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import pickle\n",
    "from scipy.optimize import minimize\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import stock_indicators as sa\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import pandas as pd\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "import yfinance as yf\n",
    "from stock_indicators import Quote\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from stock_indicators.indicators.common.enums import PeriodSize\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import GARCH\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)"
   ],
   "id": "3c90b611390ee8de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(0)"
   ],
   "id": "8846a25b7dda105f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "funds_w_names = defaultdict(pd.DataFrame)",
   "id": "8377db868b786443",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "etf_names = ['IE0005042456', 'EDMU.SW', 'EDG2.L', '36BA.DE', 'CBUS.DE', 'EWSA.AS','IE00B0M62X26','IE00B14X4Q57','IE00B1XNHC34','IE00B3FH7618','IE00B3ZW0K18','IE00B52MJY50','IE00B5M4WH52','IE00B66F4759','EMBE.L', 'DTLE.L','IE00BDFK1573','IBC5.DE','IE00BFNM3G45','IE00BHZPJ015','IE00BHZPJ452','IE00BHZPJ783',\n",
    "'IE00BLDGH553','IE00BMG6Z448','IE00BYYHSM20','IE00BYZTVT56','IE00BZ173V67','IE00BZ1NCS44','ISVIF', \"IESE.AS\", \"UEEF.DE\"]"
   ],
   "id": "34c1390d5ef60c75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for fund in etf_names:\n",
    "    print(fund)\n",
    "    a = (pd.DataFrame(yf.Ticker(fund).history(start=\"2023-01-03\", end=\"2024-01-01\")))\n",
    "    a.index = pd.to_datetime(a.index)\n",
    "    a.index = a.index.normalize()\n",
    "    full_date_range = pd.date_range(start=a.index.min(), end=a.index.max(), freq=\"D\")\n",
    "    a = a.reindex(full_date_range)\n",
    "\n",
    "    a[\"Return\"] = a[\"Close\"].pct_change()\n",
    "    a[\"Rolling Volatility\"] = a[\"Return\"].rolling(7).std()\n",
    "\n",
    "    a.fillna(method=\"bfill\", inplace=True)\n",
    "    if fund != 'ISVIF':\n",
    "        a.drop([\"Dividends\", \"Stock Splits\", \"Capital Gains\"], inplace=True, axis=1)\n",
    "    else:\n",
    "        a.drop([\"Dividends\", \"Stock Splits\"], inplace=True, axis=1)\n",
    "    a.index = pd.to_datetime(a.index, utc=True)\n",
    "    if a.index.hour[0] == 0:\n",
    "        a.index = a.index + datetime.timedelta(hours=-1)\n",
    "    a.index = a.index.normalize()\n",
    "\n",
    "    a = a.sort_index()\n",
    "\n",
    "    a[\"time_idx\"] = (a.index - a.index.min()).days\n",
    "    a[\"group_id\"] = fund\n",
    "    funds_w_names[fund] = a"
   ],
   "id": "9bae2e5bcf011ed5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# pickle.dump(funds_w_names, open('funds_w_names.pkl', 'wb'))",
   "id": "9dbcfabef3c7ada3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import pickle\n",
    "# pickle.dump(funds_w_names, open('funds_w_names_2.pkl', 'wb'))"
   ],
   "id": "1b6823a10b1ab60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "funds_w_names = pd.read_pickle('funds_w_names_2.pkl')",
   "id": "f76aeccbc6a1190e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def portfolio_factory(etfs: list):\n",
    "#     #giga_fund = pd.DataFrame()\n",
    "#     for c, ff in enumerate(etfs):\n",
    "#         #f = ff.copy()\n",
    "#\n",
    "#         ff.index = pd.to_datetime(ff.index, utc=True)\n",
    "#         if ff.index.hour[0]==0:\n",
    "#             ff.index = ff.index+datetime.timedelta(hours=-1)\n",
    "#         ff.index = ff.index.normalize()\n",
    "#\n",
    "#         ff = ff.sort_index()\n",
    "#\n",
    "#         ff[\"time_idx\"] = (ff.index - ff.index.min()).days\n",
    "#         ff[\"group_id\"] = c\n",
    "#         #f = f.reset_index()\n",
    "#\n",
    "#         #giga_fund = pd.concat([giga_fund, f], ignore_index=True)\n",
    "#     #return giga_fund"
   ],
   "id": "1b00448a032f62d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def technical_indicators_factory(etfs: list, names: list):\n",
    "    indicators_per_fund = defaultdict(pd.DataFrame)\n",
    "    figs = []\n",
    "\n",
    "    for id, f in enumerate(etfs):\n",
    "        quotes_for_f = [\n",
    "            Quote(\n",
    "                date=row.Index,\n",
    "                open=row.Open,\n",
    "                high=row.High,\n",
    "                low=row.Low,\n",
    "                close=row.Close,\n",
    "                volume=row.Volume\n",
    "            )\n",
    "            for row in f.itertuples()]\n",
    "\n",
    "\n",
    "        macd_for_f = sa.indicators.get_macd(quotes=quotes_for_f)\n",
    "        valid_macd_for_f = [\n",
    "            (result.date, result.macd, result.signal, result.histogram)\n",
    "            for result in macd_for_f\n",
    "            if result.macd is not None and result.signal is not None and result.histogram is not None\n",
    "        ]\n",
    "        macd_dates, macd_values, macd_signal, macd_histogram = zip(*valid_macd_for_f)\n",
    "\n",
    "\n",
    "        rsi_for_f = sa.indicators.get_rsi(quotes=quotes_for_f)\n",
    "        valid_rsi_for_f = [\n",
    "            (result.date, result.rsi)\n",
    "            for result in rsi_for_f\n",
    "            if result.date is not None and result.rsi is not None\n",
    "        ]\n",
    "        rsi_dates, rsi_values = zip(*valid_rsi_for_f)\n",
    "\n",
    "\n",
    "        bb_for_f = sa.indicators.get_bollinger_bands(quotes=quotes_for_f)\n",
    "        valid_bb_for_f = [\n",
    "            (result.date, result.lower_band, result.upper_band)\n",
    "            for result in bb_for_f\n",
    "            if result.date is not None and result.lower_band is not None\n",
    "        ]\n",
    "        bb_dates, bb_lower, bb_upper = zip(*valid_bb_for_f)\n",
    "\n",
    "\n",
    "        vwap_for_f = sa.indicators.get_vwap(quotes=quotes_for_f)\n",
    "        valid_vwap_for_f = [\n",
    "            (result.date, result.vwap)\n",
    "            for result in vwap_for_f\n",
    "            if result.date is not None and result.vwap is not None\n",
    "        ]\n",
    "        vwap_dates, vwap_values = zip(*valid_vwap_for_f)\n",
    "\n",
    "\n",
    "\n",
    "        pp_for_f = sa.indicators.get_pivot_points(quotes=quotes_for_f, window_size=PeriodSize.DAY)\n",
    "        valid_pp_for_f = [\n",
    "            (result.date, result.pp, result.r1, result.r2, result.r3, result.r4, result.s1, result.s2, result.s3, result.s4)\n",
    "            for result in pp_for_f\n",
    "            if result.date is not None and result.pp is not None\n",
    "        ]\n",
    "        pp_dates, pp, r1, r2, r3, r4, s1, s2, s3, s4 = zip(*valid_pp_for_f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        fig, axes = plt.subplots(5, 1, figsize=(12,6))\n",
    "\n",
    "\n",
    "        axes[0].plot(macd_dates[-20:], macd_values[-20:], label='MACD_values', linewidth=2)\n",
    "        axes[0].plot(macd_dates[-20:], macd_signal[-20:], label='MACD_signal', linewidth=2)\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        axes[1].plot(rsi_dates[-20:], rsi_values[-20:], label='RSI', linewidth=2)\n",
    "        axes[1].legend(loc='best')\n",
    "\n",
    "        axes[2].plot(bb_dates[-20:], bb_lower[-20:], label='BB_lower', linewidth=2)\n",
    "        axes[2].plot(bb_dates[-20:], bb_upper[-20:], label='BB_upper', linewidth=2)\n",
    "        axes[2].legend(loc='best')\n",
    "\n",
    "        axes[3].plot(vwap_dates[-20:], vwap_values[-20:], label='vwap', linewidth=2)\n",
    "        axes[3].legend(loc='best')\n",
    "\n",
    "        axes[4].plot(pp_dates[-20:], pp[-20:], label='pivot points', linewidth=2)\n",
    "        axes[4].plot(pp_dates[-20:], r1[-20:], label='r1', linewidth=2)\n",
    "        axes[4].plot(pp_dates[-20:], r4[-20:], label='r4', linewidth=2)\n",
    "        axes[4].plot(pp_dates[-20:], s1[-20:], label='s1', linewidth=2)\n",
    "        axes[4].plot(pp_dates[-20:], s4[-20:], label='s4', linewidth=2)\n",
    "        axes[4].legend(loc='best')\n",
    "\n",
    "\n",
    "        figs.append(fig)\n",
    "\n",
    "        f['Bullish'] = [1 if x.histogram and x.histogram > 0 else 0 for x in macd_for_f]\n",
    "        f['Bearish'] = [1 if x.histogram and x.histogram < 0 else 0 for x in macd_for_f]\n",
    "        f['isOverbought'] = [1 if x.rsi and x.rsi > 70 else 0 for x in rsi_for_f ]\n",
    "        f['isOversold'] = [1 if x.rsi and x.rsi < 30 else 0 for x in rsi_for_f ]\n",
    "\n",
    "\n",
    "        f.fillna(method=\"ffill\", inplace=True)\n",
    "        indicators_per_fund[names[id]]= f\n",
    "    return indicators_per_fund, figs"
   ],
   "id": "f42c47bebe451e96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fit(gradient_clip, drop, hidden_size, hidden_continuous_size, attention_head_size, learning_rate, epochs: int, batch_size: int, df: pd.DataFrame, independent_variables: list, target=\"Close\", training_cutoff_idx: pd.Timestamp=300, max_prediction_length=61, min_encoder_length=22):\n",
    "    training = TimeSeriesDataSet(\n",
    "        df[lambda x: x.time_idx <= training_cutoff_idx],  # Use the determined cutoff index\n",
    "        time_idx=\"time_idx\",  # Sequential time index\n",
    "        target=target,  # Target variable\n",
    "        group_ids=[\"group_id\"],  # Group identifier\n",
    "        min_encoder_length=min_encoder_length,  # Minimum input sequence length\n",
    "        max_encoder_length=334,  # Maximum input sequence length\n",
    "        min_prediction_length=1,  # Minimum forecast length\n",
    "        max_prediction_length=max_prediction_length,  # Maximum forecast length (31 for December)\n",
    "        static_categoricals=[\"group_id\"],\n",
    "        static_reals=[],  # No static real variables\n",
    "        time_varying_known_reals=independent_variables,  # Known inputs\n",
    "        time_varying_unknown_reals=[target],  # Target variable\n",
    "        target_normalizer=GroupNormalizer(groups=[\"group_id\"], transformation=\"softplus\"),\n",
    "        add_relative_time_idx=True,  # Add relative time index\n",
    "        add_target_scales=True,  # Scale the target variable\n",
    "        add_encoder_length=True,  # Include encoder length feature\n",
    "        allow_missing_timesteps=False,  # Ensure no missing steps\n",
    "    )\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(\n",
    "        training,\n",
    "        df,\n",
    "        predict=True,\n",
    "        stop_randomization=True  # Ensures no randomization in validation dataset\n",
    "    )\n",
    "\n",
    "    batch_size = batch_size\n",
    "\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "    # study = optimize_hyperparameters(\n",
    "    #     train_dataloader,\n",
    "    #     val_dataloader,\n",
    "    #     model_path=\"optuna_test\",\n",
    "    #     n_trials=200,\n",
    "    #     max_epochs=50,\n",
    "    #     gradient_clip_val_range=(0.01, 1.0),\n",
    "    #     hidden_size_range=(8, 128),\n",
    "    #     hidden_continuous_size_range=(8, 128),\n",
    "    #     attention_head_size_range=(1, 4),\n",
    "    #     learning_rate_range=(0.001, 0.1),\n",
    "    #     dropout_range=(0.1, 0.3),\n",
    "    #     trainer_kwargs=dict(limit_train_batches=30),\n",
    "    #     reduce_on_plateau_patience=4,\n",
    "    #     use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    "    # )\n",
    "    #\n",
    "    # with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    #     pickle.dump(study, fout)\n",
    "    #\n",
    "    # print(study.best_trial.params)\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=learning_rate,  # Learning rate\n",
    "        hidden_size=hidden_size,  # Model hidden size\n",
    "        attention_head_size=attention_head_size,  # Number of attention heads\n",
    "        dropout=drop,  # Dropout rate\n",
    "        hidden_continuous_size=hidden_continuous_size,  # Hidden size for continuous variables\n",
    "        output_size=7,  # Output quantiles (e.g., 10th to 90th percentile)\n",
    "        loss=QuantileLoss(),  # Loss function\n",
    "        log_interval=10,  # Log every 10 batches\n",
    "        reduce_on_plateau_patience=4,  # Reduce learning rate on plateau\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints/\",\n",
    "        filename=\"tft-model-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",  # minimize validation loss\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=8, verbose=False, mode=\"min\")\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        # logger=logger,\n",
    "        max_epochs=epochs,\n",
    "        accelerator='cuda',\n",
    "        devices=\"auto\",\n",
    "        gradient_clip_val=gradient_clip,\n",
    "        callbacks=[early_stop_callback, lr_logger],  #, checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    trainer.fit(tft, train_dataloader, val_dataloader)\n",
    "    return tft, training, val_dataloader"
   ],
   "id": "efe8180eaa630623",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predictt(tft, training, data):\n",
    "    pred_data = TimeSeriesDataSet.from_dataset(\n",
    "        dataset=training,\n",
    "        data=data,\n",
    "        predict=True,\n",
    "        stop_randomization=True)\n",
    "\n",
    "    pred_dataloader = pred_data.to_dataloader(train=False, batch_size=64 * 10, num_workers=0)\n",
    "\n",
    "    raw_predictions = tft.predict(pred_dataloader, mode=\"raw\", return_x=True)\n",
    "    predicted_median_np = raw_predictions.output.prediction[0, :, 1].detach().cpu().numpy()\n",
    "    return predicted_median_np"
   ],
   "id": "4fcfb333f4df7d07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def huber_loss(actual_val, pred_val, delta=5.0):\n",
    "    err = np.abs(actual_val - pred_val)\n",
    "    return np.mean(np.where( err <= delta, 0.5 * err**2, delta * (err - 0.5 * delta)))"
   ],
   "id": "8574c17cb6ab2f97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from decimal import Decimal\n",
    "def relative_squared_error(true, pred):\n",
    "    true = np.array([Decimal(float(x)) for x in true])\n",
    "    pred = np.array([Decimal(float(x)) for x in pred])\n",
    "\n",
    "    true_mean = np.mean(true)\n",
    "    squared_error_num = np.sum(np.square(true - pred))\n",
    "    squared_error_den = np.sum(np.square(true - true_mean))\n",
    "    rse_loss = squared_error_num / squared_error_den\n",
    "    return rse_loss\n",
    "\n",
    "def relative_absolute_error(true, pred):\n",
    "    true = np.array([Decimal(float(x)) for x in true])\n",
    "    pred = np.array([Decimal(float(x)) for x in pred])\n",
    "    true_mean = np.mean(true)\n",
    "    squared_error_num = np.sum(np.abs(true - pred))\n",
    "    squared_error_den = np.sum(np.abs(true - true_mean))\n",
    "    rae_loss = squared_error_num / squared_error_den\n",
    "    return rae_loss"
   ],
   "id": "2d89f09d6ead48a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def metrics_and_plt(df: pd.DataFrame, preds:list, date_range:pd.date_range, target=\"Close\", date = \"2023-12-01\"):\n",
    "    print(len(df))\n",
    "    print(len(preds))\n",
    "    actual_values = df[target][date:]\n",
    "    rse = relative_squared_error(actual_values, preds)\n",
    "    rae = relative_absolute_error(actual_values, preds)\n",
    "    huber = huber_loss(actual_values, preds)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(date_range, actual_values, marker=\"x\", label=\"Actual Close\", color=\"orange\")\n",
    "    plt.plot(date_range, preds, marker=\"o\", label=\"Predicted Median\")\n",
    "    plt.title(\"Predicted Values for December 01 to December 29, 2023\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Predicted Value\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return rse, rae, huber, plt"
   ],
   "id": "95005a46f490b32f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BASELINE MODEL",
   "id": "6c01803bf754d5ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "f1 = funds_w_names['EWSA.AS'][\"Close\"][:-30]",
   "id": "a745a0411236507f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = ARIMA(f1, order=(5, 2, 0))\n",
    "model_fit = model.fit()\n",
    "arima_forecast = model_fit.forecast(steps=30)\n",
    "print(\"ARIMA Forecast for the next 30 periods:\", arima_forecast)"
   ],
   "id": "d4229671f4c9a065",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "arima_rse = relative_squared_error(funds_w_names[\"EWSA.AS\"][\"Close\"][-30:], arima_forecast)\n",
    "arima_rae = relative_absolute_error(funds_w_names[\"EWSA.AS\"][\"Close\"][-30:], arima_forecast)"
   ],
   "id": "f79ed1f4037a312f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "arima_rse\n",
   "id": "efeb56ee38d5446a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sf = StatsForecast(models=[GARCH()], freq='D')\n",
    "f1=pd.DataFrame(f1)\n",
    "f1['ds'] = pd.to_datetime(f1.index)\n",
    "f1.rename(columns={'Close': 'y'}, inplace=True)\n",
    "f1['unique_id']=1"
   ],
   "id": "cbeac0df26a9058e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "garch_forecast = sf.forecast(h=30, df=f1)\n",
    "\n",
    "print(garch_forecast)"
   ],
   "id": "4764be5733acc8b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "garch_rse = relative_squared_error(funds_w_names[\"EWSA.AS\"][\"Rolling Volatility\"][-30:], garch_forecast[\"GARCH(1,1)\"])\n",
    "garch_rae = relative_absolute_error(funds_w_names[\"EWSA.AS\"][\"Rolling Volatility\"][-30:], garch_forecast[\"GARCH(1,1)\"])"
   ],
   "id": "4383ab47cd76f29a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12,6))\n",
    "\n",
    "\n",
    "axes[0].plot(funds_w_names[\"EWSA.AS\"][-30:].index, arima_forecast[-30:], label='arima_forecast', linewidth=2)\n",
    "axes[0].plot(funds_w_names[\"EWSA.AS\"][-30:].index, funds_w_names[\"EWSA.AS\"][\"Close\"][-30:], label='Close', linewidth=2)\n",
    "axes[0].legend(loc='best')\n",
    "axes[1].plot(funds_w_names[\"EWSA.AS\"][-30:].index, garch_forecast[\"GARCH(1,1)\"], label='garch_forecast', linewidth=2)\n",
    "axes[1].plot(funds_w_names[\"EWSA.AS\"][-30:].index, funds_w_names[\"EWSA.AS\"][\"Rolling Volatility\"][-30:], label='Volatility', linewidth=2)\n",
    "axes[1].legend(loc='best')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "5436d081d0387a78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_funds = defaultdict(pd.DataFrame)\n",
    "train_funds = defaultdict(pd.DataFrame)\n",
    "for i in funds_w_names:\n",
    "    test_funds[i] = funds_w_names[i][-62:]\n",
    "    train_funds[i] = funds_w_names[i][:-62]\n",
    "    #test_funds[i]= test_funds[i].drop('Close', axis=1)\n",
    "    test_funds[i][\"Close\"] = 0.0\n",
    "    test_funds[i].index = pd.to_datetime(test_funds[i].index)\n",
    "    \n",
    "    train_funds[i].index = pd.to_datetime(train_funds[i].index)\n",
    "    test_funds[i][\"time_idx\"] = (test_funds[i].index - test_funds[i].index.min()).days\n",
    "    train_funds[i][\"time_idx\"] = (train_funds[i].index - train_funds[i].index.min()).days"
   ],
   "id": "b62f4b67ac754cfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1) ONE FUND, NO INDICATORS, PRICE",
   "id": "aa8a43e607d1ad7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#IE00B52MJY50\n",
    "#IE = portfolio_factory(etfs=[funds_w_names['EWSA.AS']])"
   ],
   "id": "c1ca8dd2f10794a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_tft, training, val = fit(epochs=100, batch_size=128, df=train_funds['EWSA.AS'], independent_variables=[\"Open\", \"High\", \"Low\", \"Volume\"], drop=0.14, hidden_size=31, learning_rate=0.006, attention_head_size=4, gradient_clip=0.4, hidden_continuous_size=9, max_prediction_length=62, min_encoder_length=0)",
   "id": "a87efb6cf40e9ead",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_preds = predictt(IE_tft, training, test_funds[\"EWSA.AS\"])",
   "id": "88dd6165094e67ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_preds",
   "id": "a9542d7a205c3508",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_rse, IE_rae, IE_huber, IE_plt = metrics_and_plt(funds_w_names['EWSA.AS'], IE_preds[-31:], pd.date_range(start=\"2023-11-29\", end=\"2023-12-29\"), date=\"2023-11-28\")",
   "id": "20c73ece89dc4c3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2) ONE FUND, INDICATORS, PRICE",
   "id": "f97501dc555fb35f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_indicators, IE_indicators_plt1 = technical_indicators_factory([funds_w_names['EWSA.AS']], ['EWSA.AS'])",
   "id": "d33f888b56a7f34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analiza = IE_indicators[\"EWSA.AS\"][[\"isOversold\", \"isOverbought\", \"Bullish\", \"Bearish\", \"Return\"]]",
   "id": "bc3a73ef1eb1ab8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "corr1 = analiza.corr()\n",
    "sns.heatmap(corr1, annot=True, cmap=\"coolwarm\", fmt=\".2f\")"
   ],
   "id": "31e2a54eb373505e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "correlations = IE_indicators[\"EWSA.AS\"].corr()",
   "id": "39ef902ef6447703",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(correlations, annot=True, cmap=\"coolwarm\", fmt=\".2f\")"
   ],
   "id": "492825ea5742da02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_funds_indic = defaultdict(pd.DataFrame)\n",
    "train_funds_indic = defaultdict(pd.DataFrame)\n",
    "for i in funds_w_names:\n",
    "    test_funds_indic[i] = funds_w_names[i][-62:]\n",
    "    train_funds_indic[i] = funds_w_names[i][:-62]\n",
    "    #test_funds[i]= test_funds[i].drop('Close', axis=1)\n",
    "    test_funds_indic[i][\"Close\"] = 0.0\n",
    "    test_funds_indic[i].index = pd.to_datetime(test_funds_indic[i].index)\n",
    "\n",
    "    train_funds_indic[i].index = pd.to_datetime(train_funds_indic[i].index)\n",
    "    test_funds_indic[i][\"time_idx\"] = (test_funds_indic[i].index - test_funds_indic[i].index.min()).days\n",
    "    train_funds_indic[i][\"time_idx\"] = (train_funds_indic[i].index - train_funds_indic[i].index.min()).days"
   ],
   "id": "ee29726f53443c9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_indicators_tft, training, val = fit(epochs=100, batch_size=128, df=train_funds_indic[\"EWSA.AS\"], independent_variables=[\"Open\", \"High\", \"Low\", \"Volume\", \"isOverbought\", \"Bullish\"],gradient_clip=0.02, drop=0.15, hidden_size=31, hidden_continuous_size=12, attention_head_size=4, learning_rate=0.006, max_prediction_length=62, min_encoder_length=0)",
   "id": "9d775455a80b298f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_indicators_preds = predictt(IE_indicators_tft, training, test_funds_indic[\"EWSA.AS\"])",
   "id": "99fb07f5a717b9fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_indicators_rse, IE_indicators_rae, IE_indicators_huber, IE_indicators_plt2= metrics_and_plt(funds_w_names[\"EWSA.AS\"], IE_indicators_preds[-30:], pd.date_range(start=\"2023-11-30\", end=\"2023-12-29\"), date=\"2023-11-29\")",
   "id": "ae360d017f32d0c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_indicators_rse",
   "id": "b29d94df8da386e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_indicators[\"EWSA.AS\"][\"Close\"][\"2023-10-31\":\"2023-11-29\"]",
   "id": "782f7e7c29f53aa3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3) ONE FUND, INDICATORS, RESIDUALS",
   "id": "af0529a77520efac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "resztki = abs(IE_indicators_preds - IE_indicators[\"EWSA.AS\"][\"Close\"][\"2023-10-28\":\"2023-12-29\"])",
   "id": "2b2444a80603a2c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "resztki",
   "id": "f5d5ae57181b9898",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_scary = IE_indicators[\"EWSA.AS\"][299:].copy()\n",
    "df_scary[\"Remainders\"] = resztki"
   ],
   "id": "3c7080d0bd77cb96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_scary",
   "id": "8a11264da8fa9202",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_scary.index = pd.to_datetime(df_scary.index)  # ensure index is datetime if not already\n",
    "df_scary[\"time_idx\"] = (df_scary.index.date - df_scary.index.date[0]).astype(\"timedelta64[D]\").astype(int)\n",
    "df_scary['group_id'] = IE_indicators[\"EWSA.AS\"][\"group_id\"]"
   ],
   "id": "5ad51ac004c3d960",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "df_scary_test = df_scary[-31:]\n",
    "df_scary_train = df_scary[:-31]\n",
    "#test_funds[i]= test_funds[i].drop('Close', axis=1)\n",
    "df_scary_test[\"Remainders\"] = 0.0\n",
    "df_scary_test[\"Close\"] = 0.0\n",
    "df_scary_test.index = pd.to_datetime(df_scary_test.index)\n",
    "\n",
    "df_scary_train.index = pd.to_datetime(df_scary_train.index)\n",
    "df_scary_test[\"time_idx\"] = (df_scary_test.index - df_scary_test.index.min()).days\n",
    "df_scary_train[\"time_idx\"] = (df_scary_train.index - df_scary_train.index.min()).days"
   ],
   "id": "4f0a0b09f7dd73df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_tft_rem, training, val = fit(epochs=100, batch_size=128, df=df_scary, independent_variables=[\"Open\", \"High\", \"Low\", \"isOverbought\", \"Bullish\"], target=\"Remainders\", max_prediction_length=31, drop=0.14, hidden_size=31, learning_rate=0.006, attention_head_size=4, gradient_clip=0.4, hidden_continuous_size=9, min_encoder_length=0)",
   "id": "6b20e0831f487834",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_tft_rem_preds= predictt(IE_tft_rem, training, df_scary_test)",
   "id": "c04698e4736efa41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_model2_rse, IE_model2_rae, IE_model2_huber, IE_model2_plt= metrics_and_plt(df_scary[30:], IE_tft_rem_preds, pd.date_range(start=\"2023-11-29\", end=\"2023-12-29\"), target=\"Remainders\", date=\"2023-11-28\")",
   "id": "be45c804bbecb546",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_model2_rse",
   "id": "d41151481429bd74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_model2_rae",
   "id": "c2d296800742aef8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_model2_huber",
   "id": "c093588fb774bc69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "final_preds = IE_indicators_preds.copy()[-31:] + IE_tft_rem_preds",
   "id": "7da8c462f1268b3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "final_rse = relative_squared_error(IE_indicators[\"EWSA.AS\"][\"Close\"][-31:], final_preds)",
   "id": "b92fbede3b4e5d10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "final_rse",
   "id": "486d2e6a225447ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4) MANY FUNDS, INDICATORS, PRICE",
   "id": "984572defbe61fa1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "funds_for_hedge = [\"IE00BFNM3G45\", \"IE00BHZPJ783\", \"36BA.DE\", \"IE00B3ZW0K18\", \"EWSA.AS\"]\n",
    "funds_for_hedge2 = [\"IE0005042456\", \"EWSA.AS\", \"IE00B3ZW0K18\", \"IE00B52MJY50\", \"IE00BFNM3G45\", \"IE00BHZPJ452\", \"IE00BHZPJ783\", \"EDMU.SW\", \"IE00BYYHSM20\", \"IE00BZ173V67\", \"IE00B66F4759\", \"DTLE.L\", \"IE00BLDGH553\", \"IE00BYZTVT56\", \"EDG2.L\", \"IE00BMG6Z448\", \"EMBE.L\", \"IE00BZ1NCS44\"]\n",
    "dataframes_funds = []\n",
    "for fund in funds_for_hedge:\n",
    "    dataframes_funds.append(funds_w_names[fund])\n",
    "fund_name_fund_df, sum_plt = technical_indicators_factory(dataframes_funds, funds_for_hedge)"
   ],
   "id": "e042f684d1ece785",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_funds_indic = defaultdict(pd.DataFrame)\n",
    "train_funds_indic = defaultdict(pd.DataFrame)\n",
    "for i in funds_w_names:\n",
    "    test_funds_indic[i] = funds_w_names[i][-62:]\n",
    "    train_funds_indic[i] = funds_w_names[i][:-62]\n",
    "    #test_funds[i]= test_funds[i].drop('Close', axis=1)\n",
    "    test_funds_indic[i][\"Close\"] = 0.0\n",
    "    test_funds_indic[i].index = pd.to_datetime(test_funds_indic[i].index)\n",
    "\n",
    "    train_funds_indic[i].index = pd.to_datetime(train_funds_indic[i].index)\n",
    "    test_funds_indic[i][\"time_idx\"] = (test_funds_indic[i].index - test_funds_indic[i].index.min()).days\n",
    "    train_funds_indic[i][\"time_idx\"] = (train_funds_indic[i].index - train_funds_indic[i].index.min()).days"
   ],
   "id": "c8fdda87f94c04bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "dataframes_funds_test = []\n",
    "dataframes_funds_train = []\n",
    "dataframes_funds2 = []\n",
    "for fund in funds_for_hedge:\n",
    "\n",
    "    dataframes_funds_test.append(test_funds_indic[fund])\n",
    "    dataframes_funds_train.append(train_funds_indic[fund])\n",
    "# for fund in funds_for_hedge2:\n",
    "#     dataframes_funds2.append(funds_w_names[fund].copy())"
   ],
   "id": "fd60788763456605",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_params_for_transfromers = []\n",
    "for i, f in enumerate(dataframes_funds):\n",
    "    best_params_for_transfromers.append(fit())"
   ],
   "id": "68b135e1205d2792",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4f7857723c44cd54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_params_per_model = [\n",
    "    {'gradient_clip_val': 0.1170917991042152, 'hidden_size': 60, 'dropout': 0.11869613309043395,\n",
    "     'hidden_continuous_size': 19, 'attention_head_size': 4, 'learning_rate': 0.003814810368668662},\n",
    "    {'gradient_clip_val': 0.015691176093369945, 'hidden_size': 98, 'dropout': 0.29590810356280955,\n",
    "     'hidden_continuous_size': 37, 'attention_head_size': 6, 'learning_rate': 0.005478633587765633},\n",
    "    {'gradient_clip_val': 0.032540265870562794, 'hidden_size': 75, 'dropout': 0.28617284712308855,\n",
    "     'hidden_continuous_size': 36, 'attention_head_size': 5, 'learning_rate': 0.005579041886792157},\n",
    "    {'gradient_clip_val': 0.27402792475271975, 'hidden_size': 14, 'dropout': 0.16937295209546838,\n",
    "     'hidden_continuous_size': 10, 'attention_head_size': 4, 'learning_rate': 0.007985354289781083},\n",
    "    {'gradient_clip_val': 0.11871482116616788, 'hidden_size': 62, 'dropout': 0.16547754484324415,\n",
    "     'hidden_continuous_size': 34, 'attention_head_size': 2, 'learning_rate': 0.0012659431875886532}\n",
    "]"
   ],
   "id": "cdda8e9929b395b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "transformers=[]\n",
    "vals=[]\n",
    "trs=[]\n",
    "for i, (params, fund) in enumerate(zip(best_params_per_model, dataframes_funds_train)):\n",
    "    gradient_clip_val = params['gradient_clip_val']\n",
    "    hidden_size = params['hidden_size']\n",
    "    dropout = params['dropout']\n",
    "    hidden_continuous_size = params['hidden_continuous_size']\n",
    "    attention_head_size = params['attention_head_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "\n",
    "    independent_variables = [\"Open\", \"High\", \"Low\", \"Volume\", \"isOverbought\", \"Bullish\"]\n",
    "\n",
    "    tft, training, val = fit(\n",
    "        gradient_clip=gradient_clip_val,\n",
    "        hidden_size=hidden_size,\n",
    "        drop=dropout,\n",
    "        hidden_continuous_size=hidden_continuous_size,\n",
    "        attention_head_size=attention_head_size,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=100,                # Or set it dynamically\n",
    "        batch_size=128,           # Or set it dynamically\n",
    "        df=fund,  # Pass the fund data\n",
    "        independent_variables=independent_variables,  # Pass independent variables\n",
    "        min_encoder_length=0,\n",
    "        max_prediction_length=62,\n",
    "    )\n",
    "\n",
    "    transformers.append(tft)\n",
    "    vals.append(val)\n",
    "    trs.append(training)"
   ],
   "id": "b506e737cfecba0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for fund in fund_name_fund_df:\n",
    "#     tft, val = fit(epochs=10, batch_size=128, df=fund_name_fund_df[fund], independent_variables=[\"Open\", \"High\", \"Low\", \"isOverbought\", \"Bullish\"])\n",
    "#     transformers.append(tft)\n",
    "#     vals.append(val)"
   ],
   "id": "b6e01f743c7bfa4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pred = []\n",
    "for i in range(len(dataframes_funds_test)):\n",
    "    pred.append(predictt(transformers[i], trs[i], dataframes_funds_test[i]))"
   ],
   "id": "be38f4e690c0ef33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_many1_rse, IE_many1_rae, IE_many1_hub, IE_many1_plt2,= metrics_and_plt(funds_w_names[funds_for_hedge[4]], pred[4][-31:], pd.date_range(start=\"2023-11-29\", end=\"2023-12-29\"), date=\"2023-11-28\")",
   "id": "c88b511df6d1c9b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# GIGAFUND_NORM = GIGAFUND.copy()\n",
    "# \n",
    "# columns_to_normalize = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Return\", \"Rolling Volatility\"]\n",
    "# \n",
    "# GIGAFUND_NORM[columns_to_normalize] = (\n",
    "#     GIGAFUND_NORM[columns_to_normalize]\n",
    "#     .apply(lambda col: (col - col.min()) / (col.max() - col.min()), axis=0)\n",
    "# )"
   ],
   "id": "e031d8162306bbe2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# duplicate_rows = GIGAFUND_NORM[GIGAFUND_NORM.duplicated(subset=[\"time_idx\", \"group_id\"], keep=False)]\n",
    "# print(duplicate_rows)"
   ],
   "id": "b4dab9eecd8ff6c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# gftft, gftr, gfv = fit(epochs=10, batch_size=128, lr=1e-3, dropout=0.1, df=GIGAFUND_NORM, independent_variables=[\"Open\", \"High\", \"Low\", \"isOverbought\", \"Bullish\"], target=\"Close\")",
   "id": "616148dfd7bc99f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# IE_indicators_preds = predictt(gftft, gftr, gfv)",
   "id": "559cb43917ba1aca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# normalized = dataframes_funds[1][columns_to_normalize] = (\n",
    "#     dataframes_funds[1][columns_to_normalize]\n",
    "#     .apply(lambda col: (col - col.min()) / (col.max() - col.min()), axis=0)\n",
    "# )"
   ],
   "id": "92aedff51f616074",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# mse, plt, mape, huber = metrics_and_plt(dataframes_funds[1], IE_indicators_preds, pd.date_range(start=\"2023-10-29\", end=\"2023-12-29\"), date=\"2023-10-29\")",
   "id": "3ab261ed9dd21ea9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5) MANY FUNDS, INDICATORS, RESIDUALS",
   "id": "652079f46ab42e7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "resztkiii = []\n",
    "for i in range(len(funds_for_hedge)):\n",
    "    resztkiii.append(abs(pred[i] - fund_name_fund_df[funds_for_hedge[i]][\"Close\"][\"2023-10-28\":\"2023-12-29\"]))"
   ],
   "id": "bad2af78cc1645a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "residualsss=[]\n",
    "residualsss_test= []\n",
    "residualsss_train= []\n",
    "for i in range(len(funds_for_hedge)):\n",
    "    residualsss.append(funds_w_names[funds_for_hedge[i]][299:].copy())\n",
    "    residualsss[i][\"Remainders\"] = resztkiii[i]\n",
    "    residualsss_test.append(residualsss[i][-31:].copy())\n",
    "    residualsss_train.append(residualsss[i][:-31].copy())\n",
    "    residualsss_test[i][\"Close\"] = 0.0\n",
    "    residualsss_test[i][\"Remainders\"] = 0.0\n",
    "    residualsss_test[i].index = pd.to_datetime(residualsss_test[i].index)\n",
    "    residualsss_train[i].index = pd.to_datetime(residualsss_train[i].index)\n",
    "    \n",
    "    residualsss_test[i][\"time_idx\"] = (residualsss_test[i].index - residualsss_test[i].index.min()).days.astype(int)\n",
    "    residualsss_train[i][\"time_idx\"] = (residualsss_train[i].index - residualsss_train[i].index.min()).days.astype(int)"
   ],
   "id": "3b50748f1948ad46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "transformers_res=[]\n",
    "vals_res=[]\n",
    "trs_res=[]\n",
    "for i in range(len(funds_for_hedge)):\n",
    "    tft, training, val = fit(epochs=100, batch_size=128, df=residualsss_train[i], independent_variables=[\"Open\", \"High\", \"Low\", \"Volume\", \"isOverbought\", \"Bullish\"], target=\"Remainders\", max_prediction_length=30, min_encoder_length=1, gradient_clip=0.02, drop=0.15, hidden_size=31, hidden_continuous_size=12, attention_head_size=4, learning_rate=0.006)\n",
    "    transformers_res.append(tft)\n",
    "    vals_res.append(val)\n",
    "    trs_res.append(training)"
   ],
   "id": "8105110cdd1a65bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res_preds = []\n",
    "for i in range(len(funds_for_hedge)):\n",
    "    res_preds.append(predictt(transformers_res[i], trs_res[i], residualsss_test[i]))"
   ],
   "id": "6fcc10412fa7711d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_many1_res_rse, IE_many1_res_rae, IE_many1_res_huber, IE_many1_res_plt = metrics_and_plt(residualsss[0][-30:], res_preds[0],pd.date_range(start=\"2023-11-30\", end=\"2023-12-29\"), target=\"Remainders\", date=\"2023-11-28\")   ",
   "id": "795943975a6105ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(IE_many1_res_rse, IE_many1_res_rae, IE_many1_res_huber)",
   "id": "9f59579dc6e2643e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6) FINAL RESULT PRICE + RESIDUALS",
   "id": "e6670219240544c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_many_preds = []\n",
    "for i in range(len(funds_for_hedge)):\n",
    "    final_many_preds.append(pred[i].copy()[-30:] + res_preds[i])"
   ],
   "id": "945dba9f237d424c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "IE_final_many1_res_rse, IE_final_many1_res_rae, IE_final_many1_res_huber, IE_final_many1_res_plt = metrics_and_plt(funds_w_names[funds_for_hedge[0]][-30:], final_many_preds[0],pd.date_range(start=\"2023-11-30\", end=\"2023-12-29\"), target=\"Close\", date=\"2023-11-28\")",
   "id": "aa41c76b84b49eaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MONTER CARLOS PREDICTING RISK",
   "id": "8746f4fc7f9f7320"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calc_drift(fund):\n",
    "    log_returns = np.log(1+fund[\"Close\"].pct_change())\n",
    "    log_returns.fillna(value=0, inplace=True)\n",
    "    avg_pdr = log_returns.mean()\n",
    "    var = log_returns.var()\n",
    "    drift = avg_pdr-(.5*var)\n",
    "\n",
    "    return drift"
   ],
   "id": "c208ac72d0b63867",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def monte_carlo_sim(fund):\n",
    "    drift = calc_drift(fund)\n",
    "    log_returns = np.log(1+fund[\"Close\"].pct_change())\n",
    "    log_returns.fillna(value=0, inplace=True)\n",
    "    stdev=log_returns.std()\n",
    "    days=362\n",
    "    trials=100\n",
    "    Z = norm.ppf(np.random.rand(days,trials))\n",
    "    daily_returns=np.exp(np.array(drift) + np.array(stdev) * Z)\n",
    "    price_paths = np.zeros_like(daily_returns)\n",
    "    price_paths[0] = fund[\"Close\"].iloc[-1]\n",
    "    for i in range(1,days):\n",
    "        price_paths[i] = price_paths[i-1] * daily_returns[i]\n",
    "    return price_paths"
   ],
   "id": "aa30c81769788dfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "drifts=calc_drift(funds_w_names[\"EWSA.AS\"])\n",
    "pp=monte_carlo_sim(funds_w_names[\"EWSA.AS\"])"
   ],
   "id": "583f06f5786c5051",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def volatility_mc(price_paths: pd.DataFrame):\n",
    "    pp_df = pd.DataFrame(price_paths)\n",
    "    pp_df= pp_df.pct_change()\n",
    "    pp_df.fillna(value=0, inplace=True)\n",
    "    pp_df=pp_df.rolling(7).std()\n",
    "    pp_df.fillna(method=\"bfill\", inplace=True)\n",
    "    return np.mean(pp_df)"
   ],
   "id": "ac542dd191a53e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "volatility_mc(pp)",
   "id": "52e2643832f9a55b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(pp)"
   ],
   "id": "c36f42d9a00afe20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TEZ DO KOSZA",
   "id": "ebd1e71253d1e173"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list_of_5_df_funds = []",
   "id": "21930338decf86f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def nearest_positive_definite(matrix):\n",
    "    P = matrix.copy()\n",
    "    eigvals, eigvecs = np.linalg.eigh(P)\n",
    "    eigvals[eigvals < 0] = 1e-10\n",
    "    return eigvecs @ np.diag(eigvals) @ eigvecs.T"
   ],
   "id": "151d1474b222074",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "variances = {}\n",
    "covs_means = []\n",
    "for k,v in funds_w_names.items():\n",
    "    variances[k] = v[\"Rolling Volatility\"]\n",
    "variances = pd.DataFrame(variances)\n",
    "r = variances.pct_change()\n",
    "covariance_matrix = r.cov()\n",
    "mean_returns = r.mean()"
   ],
   "id": "308f7d0d5196e392",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not np.all(np.linalg.eigvals(covariance_matrix) > 0):\n",
    "    print(\"Covariance matrix not positive definite. Adding regularization.\")\n",
    "    covariance_matrix = nearest_positive_definite(covariance_matrix)"
   ],
   "id": "4f410d378fab6d79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#to be filled with real data\n",
    "portfolio_weights = np.random.random(len(mean_returns)) # distribution of etfs in portfolio\n",
    "portfolio_weights /= np.sum(portfolio_weights)"
   ],
   "id": "3a4f438d6a2cd272",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "initial_portfoloio = 0\n",
    "for i, v in enumerate(funds_w_names.values()):\n",
    "    initial_portfoloio += portfolio_weights[i] * v[\"Close\"][i]"
   ],
   "id": "fc6ccab90c89185e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "days = 60\n",
    "simulations = 100\n",
    "portfolio_sims = np.full(shape=(days, simulations), fill_value=0.0) # default\n",
    "mean_matrix = np.full(shape=(days, len(portfolio_weights)), fill_value=mean_returns).T # days x all etfs"
   ],
   "id": "ada84324f00035a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "initial_portfoloio",
   "id": "a2ad86d38b922d5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for s in range(0, simulations):\n",
    "    Z = np.random.normal(size=(days, len(portfolio_weights)))\n",
    "    L = np.linalg.cholesky(covariance_matrix)\n",
    "    daily_returns = mean_matrix + np.inner(L, Z)\n",
    "    portfolio_sims[:, s] = np.cumprod(np.inner(portfolio_weights, daily_returns.T)+1)*initial_portfoloio\n",
    "\n",
    "plt.plot(portfolio_sims)\n",
    "plt.ylabel(\"Portfolio Simulations\")"
   ],
   "id": "ecf4d7e27c9122ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "expected volatility",
   "id": "d244aa0b998b0e1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "portfolio_returns = portfolio_sims[-1] / portfolio_sims[0] - 1  # final value over initial value gives portfolio returns\n",
    "portfolio_volatility = np.std(portfolio_returns)\n",
    "print(f\"Portfolio Volatility: {portfolio_volatility:.4f}\")"
   ],
   "id": "317ed4aac1416795",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(portfolio_sims)\n",
    "plt.title(\"Monte Carlo Simulations of Portfolio Value\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Portfolio Value\")\n",
    "plt.show()"
   ],
   "id": "a0e0fa6e382aac99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "expected return",
   "id": "c85b73b396741ce8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "discounted_prices = [sim[-1]*0.96 for sim in portfolio_sims]",
   "id": "dcea8a578400d3e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "avg = np.average(discounted_prices)",
   "id": "1f166ee20de2b2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MONTE CARLO EVALUATION FUNCITON",
   "id": "24cfb1cce3e17ef5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_robustness(paths, num_trials, tol=0.01):\n",
    "    means = []\n",
    "    for i in range(num_trials):\n",
    "        np.random.seed(i)\n",
    "        new_paths = np.random.choice(paths, size=len(paths), replace=True)\n",
    "        means.append(np.mean(new_paths))\n",
    "    glob_mean = np.mean(means)\n",
    "    devs = np.abs(np.array(means) - glob_mean)\n",
    "    return max(0, 1 - (np.mean(devs) / (tol*glob_mean)))"
   ],
   "id": "956c390bfca0b14c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_convergence_rate(paths, true_val):\n",
    "    cum_means = np.cumsum(paths)/np.arange(1, len(paths)+1)\n",
    "    difs = np.abs(cum_means - true_val)\n",
    "    final = difs[-1]\n",
    "    avg = np.mean(difs)\n",
    "    if avg == 0.0:\n",
    "        return 1.0\n",
    "    return max(0, 1-(final/avg))"
   ],
   "id": "5bb01c95a96d6ae9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def rate_mc_simulation(paths, true_value, runtime, weights):\n",
    "    mean_estimate = np.mean(paths)\n",
    "    variance = np.var(paths)\n",
    "    bias = mean_estimate - np.mean(true_value)\n",
    "    mse = bias**2 + variance\n",
    "    efficiency = 1 / (runtime * mse)\n",
    "\n",
    "    robustness = test_robustness(paths.mean(axis=1), 150)\n",
    "\n",
    "    convergence_rate = compute_convergence_rate(paths.mean(axis=1), true_value)\n",
    "\n",
    "    score = (\n",
    "            weights[0] * convergence_rate +\n",
    "            weights[1] * (1 / np.std(paths)) +\n",
    "            weights[2] * (1 / abs(bias)) +\n",
    "            weights[3] * (1 / variance) +\n",
    "            weights[4] * efficiency +\n",
    "            weights[5] * robustness)\n",
    "    return score"
   ],
   "id": "a3c4a8bd2bf32b4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "weights=[1/6,1/6,1/6,1/6,1/6,1/6]\n",
    "pd.Series(pp['EWSA.AS'].mean(axis=1))"
   ],
   "id": "f0ec0f86195ceabc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print((rate_mc_simulation(pp['EWSA.AS'], funds_w_names[\"EWSA.AS\"][\"Close\"], 0.1, weights)))",
   "id": "4c07e8928e6f1495",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "REDESIGN PORTFOLIO",
   "id": "b69f0919069f2fd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "list of predictions for each fund in portfolio\n",
    "'''\n",
    "portfolio = funds_for_hedge\n",
    "preds_for_portfolio = []\n",
    "for i in portfolio:\n",
    "    preds_for_portfolio.append(predictt(29, funds_w_names[i]))"
   ],
   "id": "734451b62683ba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "calculate ratio of a single fund based on provided predictions\n",
    "'''\n",
    "def calculate_funds_ratio(funds_preds, fund: pd.DataFrame):\n",
    "    funds_return = fund[\"Close\"].iloc[-1] - funds_preds[\"Close\"].iloc[-1]\n",
    "    funds_risk = volatility_mc(monte_carlo_sim(fund))\n",
    "    data = yf.Ticker(\"^TNX\").history(period=\"1d\")\n",
    "    risk_free_rate = data['Close'].iloc[-1] / 100\n",
    "    sharpe_ratio = (funds_return / len(fund)) - risk_free_rate / funds_risk\n",
    "    return sharpe_ratio, funds_return #funds_return/funds_risk"
   ],
   "id": "eefac5c01a18da41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def redistribute_funds(funds_dfs:list, weights: list):\n",
    "    \"\"\"\n",
    "    for a given portfolio, calculate risk and return for next month for each asset and decide whether to buy more of it\n",
    "    funds_dfs is a list of funds dataframes based on portfolio\n",
    "    weights is a list of weights for each funds\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(w):\n",
    "        portfolio_score = 0\n",
    "        for i in (range(len(funds_dfs))):\n",
    "            ratio, returnn, preds = calculate_funds_ratio(preds_for_portfolio, funds_dfs[i])\n",
    "            portfolio_score += ratio * w[i]\n",
    "            return -portfolio_score\n",
    "\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    bounds = [(0,1)] * len(funds_dfs)\n",
    "    result = minimize(objective, np.ndarray(weights), method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    optimized_weights = result.x\n",
    "    optimized_score = result.fun\n",
    "\n",
    "    returns = [calculate_funds_ratio(preds_for_portfolio[i], funds_dfs[i])[1] for i in range(len(portfolio))]\n",
    "    final_gain = optimized_weights*returns\n",
    "\n",
    "    return optimized_weights, optimized_score, final_gain"
   ],
   "id": "d89ad29be5849a40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "works for small Ns, max 10",
   "id": "27b458463a7240fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def design_optimal_portfolio(N: int):\n",
    "    available_funds = list(funds_w_names.keys())\n",
    "    fund_combinations = list(combinations(available_funds, N))\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_weights = None\n",
    "    best_selected_funds = None\n",
    "    best_final_gains = None\n",
    "\n",
    "    for selected_funds in fund_combinations:\n",
    "\n",
    "        funds_dfs = [funds_w_names[fund] for fund in selected_funds]\n",
    "\n",
    "\n",
    "        preds_for_selected_funds = []\n",
    "        for fund in selected_funds:\n",
    "            preds_for_selected_funds.append(predictt(29, funds_w_names[fund]))\n",
    "\n",
    "\n",
    "        def objective(w):\n",
    "            portfolio_score = 0\n",
    "            for i in range(len(funds_dfs)):\n",
    "                ratio, returnn, _ = calculate_funds_ratio(preds_for_selected_funds[i], funds_dfs[i])\n",
    "                portfolio_score += ratio * w[i]\n",
    "            return -portfolio_score\n",
    "\n",
    "\n",
    "        constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "        bounds = [(0, 1)] * N\n",
    "\n",
    "        initial_weights = np.ones(N) / N\n",
    "\n",
    "        result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "        optimized_weights = result.x\n",
    "        optimized_score = -result.fun\n",
    "\n",
    "        returns = [calculate_funds_ratio(preds_for_selected_funds[i], funds_dfs[i])[1] for i in range(N)]\n",
    "        final_gains = np.dot(optimized_weights, returns)\n",
    "\n",
    "        if optimized_score > best_score:\n",
    "            best_score = optimized_score\n",
    "            best_weights = optimized_weights\n",
    "            best_selected_funds = selected_funds\n",
    "            best_final_gains = final_gains\n",
    "\n",
    "    return best_weights, best_score, best_final_gains, best_selected_funds"
   ],
   "id": "ff090245d287f74e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "N = 5\n",
    "best_weights, best_score, best_final_gains, best_selected_funds = design_optimal_portfolio(N)\n",
    "\n",
    "print(\"Selected funds:\", best_selected_funds)\n",
    "print(\"Optimized weights:\", best_weights)\n",
    "print(\"Best gain:\", best_final_gains)"
   ],
   "id": "e6c07244aef2a959",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "works for large Ns",
   "id": "2e5e192e622ba568"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "from scipy.optimize import minimize\n",
    "from itertools import combinations\n",
    "\n",
    "def calculate_fitness(preds_for_selected_funds, funds_dfs, weights):\n",
    "    portfolio_score = 0\n",
    "    for i in range(len(funds_dfs)):\n",
    "        ratio, returnn, _ = calculate_funds_ratio(preds_for_selected_funds[i], funds_dfs[i])\n",
    "        portfolio_score += ratio * weights[i]\n",
    "    return portfolio_score\n",
    "\n",
    "def genetic_algorithm(funds_w_names, N, population_size=50, generations=100, mutation_rate=0.1, crossover_rate=0.8):\n",
    "    available_funds = list(funds_w_names.keys())\n",
    "    fund_combinations = list(combinations(available_funds, N))\n",
    "\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        selected_funds = random.sample(fund_combinations, 1)[0]\n",
    "        weights = np.random.rand(N)\n",
    "        weights /= np.sum(weights)\n",
    "        population.append((selected_funds, weights))\n",
    "\n",
    "    best_solution = None\n",
    "    best_fitness = -np.inf\n",
    "\n",
    "    for generation in range(generations):\n",
    "        fitness_scores = []\n",
    "        preds_for_selected_funds_list = []\n",
    "        funds_dfs_list = []\n",
    "\n",
    "        for selected_funds, _ in population:\n",
    "            funds_dfs = [funds_w_names[fund] for fund in selected_funds]\n",
    "            preds_for_selected_funds = []\n",
    "            for fund in selected_funds:\n",
    "                preds_for_selected_funds.append(predictt(29, funds_w_names[fund]))\n",
    "            preds_for_selected_funds_list.append(preds_for_selected_funds)\n",
    "            funds_dfs_list.append(funds_dfs)\n",
    "\n",
    "        for i in range(population_size):\n",
    "            selected_funds, weights = population[i]\n",
    "            fitness = calculate_fitness(preds_for_selected_funds_list[i], funds_dfs_list[i], weights)\n",
    "            fitness_scores.append(fitness)\n",
    "            if fitness > best_fitness:\n",
    "                best_fitness = fitness\n",
    "                best_solution = (selected_funds, weights)\n",
    "\n",
    "        selected_parents = []\n",
    "        fitness_sum = np.sum(fitness_scores)\n",
    "        prob = [score / fitness_sum for score in fitness_scores]\n",
    "\n",
    "        for _ in range(population_size):\n",
    "            selected_parent = random.choices(population, prob)[0]\n",
    "            selected_parents.append(selected_parent)\n",
    "\n",
    "        new_population = []\n",
    "        for i in range(0, population_size, 2):\n",
    "            parent1, parent2 = selected_parents[i], selected_parents[i+1]\n",
    "            if random.random() < crossover_rate:\n",
    "                crossover_point = random.randint(1, N-1)  # Crossover at a random point\n",
    "                offspring1 = (parent1[0][:crossover_point] + parent2[0][crossover_point:],\n",
    "                              np.concatenate((parent1[1][:crossover_point], parent2[1][crossover_point:])))\n",
    "                offspring2 = (parent2[0][:crossover_point] + parent1[0][crossover_point:],\n",
    "                              np.concatenate((parent2[1][:crossover_point], parent1[1][crossover_point:])))\n",
    "                new_population.extend([offspring1, offspring2])\n",
    "            else:\n",
    "                new_population.extend([parent1, parent2])\n",
    "\n",
    "        for i in range(population_size):\n",
    "            if random.random() < mutation_rate:\n",
    "                selected_funds, weights = new_population[i]\n",
    "                mutation_idx = random.randint(0, N-1)\n",
    "                weights[mutation_idx] = random.random()\n",
    "                weights /= np.sum(weights)  # Normalize weights to sum to 1\n",
    "                new_population[i] = (selected_funds, weights)\n",
    "\n",
    "        population = new_population\n",
    "\n",
    "    return best_solution, best_fitness"
   ],
   "id": "a08fa48cc7803331",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "N = 5\n",
    "best_solution, best_fitness = genetic_algorithm(funds_w_names, N)\n",
    "\n",
    "selected_funds, optimized_weights = best_solution\n",
    "print(\"Selected funds:\", selected_funds)\n",
    "print(\"Optimized weights:\", optimized_weights)\n",
    "print(\"Best fitness (risk-adjusted return):\", best_fitness)"
   ],
   "id": "9afe467fab031b40",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
